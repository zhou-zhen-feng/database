{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c91eaaf-365d-4916-b4ef-7dfbe3cc862c",
   "metadata": {},
   "source": [
    "### å…¨éƒ¨è¯­æ–™åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525da5d6-3b4a-4bb4-914a-a411ffb333fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import json\n",
    "import traceback\n",
    "import threading\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f656fa93-f63c-4c0f-80a7-d70741683f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥æ•°æ®\n",
    "df_1800_1910_part1 = pd.read_csv(r\"D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\1800_1910_american_story_part1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa2a60f-a35e-4b5f-bf2c-df1d69a43733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"By ORDER OF COURT,\\nnNE third part of piece Of Flats\\n~ Land, in Portland, in she county of Cumberland,\\nwith one third part of the wharf thereon Handing,\\nmonty called Weeks wharf.\\n\\n\\nAlto, another piece of Flats Land,\\nituae nearly oppoGte the houie of Mr.. Thomas Beck\\nPortland sforcad. the fame being the fare of Flats\\nbelonging to piece of land fold by THOMAS cunMIso\\nlate ct aid Portland, merchant deceaed, to Samuel\\nM Quincy.\\n\\n\\nALTO, large lot of Land fituate in\\nin King .frwt in the fame Portland. with dwelling\\nHoue ianeins thereon, aud the other buildings the\\nfame, being the dwelling houfe OF the fad Thomas,\\nexcept that part of faid lot and buildings which\\nft on to the fubcriber. widow o faid deeesied her\\ndover in her faid hulband's .eaate.\\n\\n\\nALTO, the remainder of faid lot lait\\nmentioned. and the buildings and sppurtenances fub-\\nje only to the life effA,e OF the fad tenant dover.\\nThe above being all the real elfate\\nwhereof the faid Thomas died feized and ponefed.\\nwill be fold on the day above mentioned, the pre-\\nmifes; or 70 much thereof a. wiz be fuf6cent pay\\nthe Jun debts Of the fad dtceafed, With incident1\\ncharges.\\n\\n\\nD&fss at Portl~rf. n. 1,ld\\nFsyrs1r,, .4~s D.~;ni 1900.\\n\\n\\nEleonora Cumming,\\nddssjsiarsty;x On fs;j Mafs.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1800_1910_part1[\"Article_body\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d212b06d-149f-4553-89d4-3a15b9993ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŠŠåˆ—è¡¨ä¸­ï¼Œæ¯ä¸€ä¸ªå­—å…¸å…ƒç´ ä¸­çš„white_contextså’Œblack_contextsåˆå¹¶æˆä¸€ä¸ªtext,æ²¡æœ‰çš„è¯è·³è¿‡\n",
    "# from tqdm import tqdm \n",
    "# merged_blackswhites_texts = []\n",
    "\n",
    "# for item in tqdm(data):\n",
    "#     # åˆå¹¶ white_contexts å’Œ black_contexts\n",
    "#     contexts = item['white_contexts'] + item['black_contexts']\n",
    "#     if contexts:  # å¦‚æœä¸ä¸ºç©º\n",
    "#         merged_text = ' '.join(contexts)  # åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²\n",
    "#         merged_blackswhites_texts.append(merged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93c573f-42c1-413a-92c3-ff49b7e1a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(merged_blackswhites_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245f227-021c-461f-992b-013860e9fdd2",
   "metadata": {},
   "source": [
    "### ç­›é€‰,60%ä»¥ä¸Šæ˜¯è‹±æ–‡è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67466ca9-2fa0-4657-a9ff-8f08bed0d9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¸‹è½½\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('state_union')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e577710-6ec1-44ff-8abd-c02a1ca514a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from match_function import process_chunk, init_worker\n",
    "\n",
    "def parallel_filter_texts_pipe(texts, batch_size=10000, chunk_size=1000, max_workers=32):\n",
    "    \"\"\"å¹¶è¡Œæ‰¹é‡å¤„ç†æ–‡æœ¬ï¼Œè¿”å›ç­›é€‰åçš„ç»“æœ\"\"\"\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "    filtered_all = []\n",
    "\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start:start + batch_size]\n",
    "        chunks = [batch[i:i + chunk_size] for i in range(0, len(batch), chunk_size)]\n",
    "\n",
    "        with mp.Pool(processes=max_workers, initializer=init_worker) as pool:\n",
    "            results = [\n",
    "                pool.apply_async(process_chunk, args=(chunk, progress_queue))\n",
    "                for chunk in chunks\n",
    "            ]\n",
    "\n",
    "            with tqdm(total=len(batch), desc=f\"Filtering [{start}-{start + len(batch)}]\") as pbar:\n",
    "                for _ in range(len(batch)):\n",
    "                    progress_queue.get()\n",
    "                    pbar.update(1)\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        for r in results:\n",
    "            filtered_all.extend([res for res in r.get() if res is not None])\n",
    "\n",
    "    return filtered_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef26f6ee-da17-4a33-ab41-67b34f137819",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts = df_1800_1910_part1[\"Article_body\"].tolist()\n",
    "    \n",
    "    print(1)\n",
    "    \n",
    "    filtered_texts = parallel_filter_texts_pipe(\n",
    "        texts,\n",
    "        batch_size=100000,\n",
    "        chunk_size=10000,\n",
    "        max_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c23c79ea-2f45-461e-bf61-526634f8d8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29754890"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86193205-4948-44c4-bedd-6eddc531d585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"By ORDER OF COURT, nNE third part of piece Of Flats ~ Land, in Portland, in she county of Cumberland, with one third part of the wharf thereon Handing, monty called Weeks wharf.   Alto, another piece of Flats Land, ituae nearly oppoGte the houie of Mr.. Thomas Beck Portland sforcad. the fame being the fare of Flats belonging to piece of land fold by THOMAS cunMIso late ct aid Portland, merchant deceaed, to Samuel M Quincy.   ALTO, large lot of Land fituate in in King .frwt in the fame Portland. with dwelling Houe ianeins thereon, aud the other buildings the fame, being the dwelling houfe OF the fad Thomas, except that part of faid lot and buildings which ft on to the fubcriber. widow o faid deeesied her dover in her faid hulband's .eaate.   ALTO, the remainder of faid lot lait mentioned. and the buildings and sppurtenances fubje only to the life effA,e OF the fad tenant dover. The above being all the real elfate whereof the faid Thomas died feized and ponefed. will be fold on the day above mentioned, the premifes; or 70 much thereof a. wiz be fuf6cent pay the Jun debts Of the fad dtceafed, With incident1 charges.   D&fss at Portl~rf. n. 1,ld Fsyrs1r,, .4~s D.~;ni 1900.   Eleonora Cumming, ddssjsiarsty;x On fs;j Mafs.\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_texts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "936a53b8-4f05-4b0e-85e3-c845a4350489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# ä¿å­˜\n",
    "with open( r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\filtered_texts_1.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66756ee9-f9eb-42c0-bf0f-75f10b588635",
   "metadata": {},
   "source": [
    "### æ¸…æ´—æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730fa463-ddd5-4986-866e-1fd7171f76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®å¤ªå¤§ åˆ†å¼€æ¸…æ´—\n",
    "import pickle\n",
    "\n",
    "# è¯»å–åŸå§‹æ•°æ®\n",
    "with open(r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\filtered_texts_1.pkl', 'rb') as f:\n",
    "    texts = pickle.load(f)\n",
    "\n",
    "# è®¡ç®—æ¯ä¸€ä»½çš„é•¿åº¦\n",
    "total_len = len(texts)\n",
    "num_parts = 5\n",
    "part_len = total_len // num_parts\n",
    "\n",
    "# åˆ†äº”ä»½ï¼ˆæœ€åä¸€ä»½åŒ…å«å‰©ä½™ï¼‰\n",
    "texts_part1 = texts[0:part_len]\n",
    "texts_part2 = texts[part_len:2*part_len]\n",
    "texts_part3 = texts[2*part_len:3*part_len]\n",
    "texts_part4 = texts[3*part_len:4*part_len]\n",
    "texts_part5 = texts[4*part_len:]\n",
    "\n",
    "# ä¿å­˜äº”ä»½\n",
    "base_path = r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—'\n",
    "\n",
    "with open(base_path + r'\\filtered_texts_1_subpart1.pkl', 'wb') as f:\n",
    "    pickle.dump(texts_part1, f)\n",
    "\n",
    "with open(base_path + r'\\filtered_texts_1_subpart2.pkl', 'wb') as f:\n",
    "    pickle.dump(texts_part2, f)\n",
    "\n",
    "with open(base_path + r'\\filtered_texts_1_subpart3.pkl', 'wb') as f:\n",
    "    pickle.dump(texts_part3, f)\n",
    "\n",
    "with open(base_path + r'\\filtered_texts_1_subpart4.pkl', 'wb') as f:\n",
    "    pickle.dump(texts_part4, f)\n",
    "\n",
    "with open(base_path + r'\\filtered_texts_1_subpart5.pkl', 'wb') as f:\n",
    "    pickle.dump(texts_part5, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb873286-2623-4393-bfc8-d4ccee771bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_cleaner.py\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "import contractions\n",
    "from match_function import process_text_2,init_worker_2,is_valid_token\n",
    "\n",
    "def parallel_clean_texts(texts, batch_size=100000, chunk_size=1000, max_workers=32):\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "    finally_results = []\n",
    "\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start:start + batch_size]\n",
    "        chunks = [batch[i:i + chunk_size] for i in range(0, len(batch), chunk_size)]\n",
    "\n",
    "        with mp.Pool(processes=max_workers, initializer=init_worker_2) as pool:\n",
    "            async_results = [\n",
    "                pool.apply_async(process_text_2, args=(chunk, progress_queue))\n",
    "                for chunk in chunks]\n",
    "\n",
    "            # æ˜¾ç¤ºå­ä»»åŠ¡å¤„ç†è¿›åº¦\n",
    "            with tqdm(total=len(batch), desc=f\"Filtering [{start}-{start + len(batch)}]\") as pbar:\n",
    "                for _ in range(len(batch)):\n",
    "                    progress_queue.get()\n",
    "                    pbar.update(1)\n",
    "\n",
    "            # âœ… åœ¨ Pool å†…è·å–ç»“æœï¼Œé¿å…å¡æ­»\n",
    "            for r in tqdm(async_results, desc=\"Collecting batch results\"):\n",
    "                try:\n",
    "                    results = r.get(timeout=60)  # åŠ ä¸Š timeout æ›´ç¨³\n",
    "                    if results:\n",
    "                        finally_results.extend(results)\n",
    "                except Exception as e:\n",
    "                    # print(f\"[WARNING] Skipped result due to error: {e}\")\n",
    "                    continue\n",
    "\n",
    "    return finally_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b2466e-70e4-40d2-948d-22581216c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # texts = df_1800_1910_select_2['Article_body'].tolist()\n",
    "    import pickle\n",
    "    with open(r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\filtered_texts_1_subpart5.pkl', 'rb') as f:\n",
    "       texts = pickle.load(f)\n",
    "\n",
    "    print(1)\n",
    "\n",
    "    cleaned_sentences_tokens = parallel_clean_texts(texts,batch_size=100000, chunk_size=1000, max_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e4af48-b3e0-422a-8198-07da70b8a839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28400169"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_sentences_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8262e711-5bfb-4a72-ad05-48d0f0e87661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜\n",
    "import pickle\n",
    "with open( r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\cleaned_texts_subpart5.pkl', 'wb') as f:\n",
    "    \n",
    "    pickle.dump(cleaned_sentences_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be534e-6443-4232-a4e3-bd762665efe0",
   "metadata": {},
   "source": [
    "#### é’ˆå¯¹æ–‡æœ¬åˆ—è¡¨çš„äºŒæ¬¡æ¸…æ´— (åªç”¨ä¸€æ¬¡ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c152bf-6256-4641-bccd-53a7580e98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# from match_function import clean_chunk,is_valid_token_1,clean_sentence\n",
    "# import re\n",
    "\n",
    "# def parallel_cleaning(all_sentences, batch_size=100000, chunk_size=1000, max_workers=30):\n",
    "#     manager = mp.Manager()\n",
    "#     progress_queue = manager.Queue()\n",
    "#     cleaned_all = []\n",
    "\n",
    "#     for batch_start in range(0, len(all_sentences), batch_size):\n",
    "#         batch = all_sentences[batch_start:batch_start + batch_size]\n",
    "#         chunks = [batch[i:i + chunk_size] for i in range(0, len(batch), chunk_size)]\n",
    "\n",
    "#         with mp.Pool(processes=max_workers) as pool:\n",
    "#             results = []\n",
    "#             for chunk in chunks:\n",
    "#                 res = pool.apply_async(clean_chunk, args=(chunk, progress_queue))\n",
    "#                 results.append(res)\n",
    "\n",
    "#             # Real-time progress bar per sentence\n",
    "#             with tqdm(total=len(batch), desc=f\"Filtering [{batch_start}-{batch_start + len(batch)}]\", unit=\"sents\") as pbar:\n",
    "#                 for _ in range(len(batch)):\n",
    "#                     progress_queue.get()\n",
    "#                     pbar.update(1)\n",
    "\n",
    "#             # Collect results\n",
    "#             for r in results:\n",
    "#                 cleaned_all.extend(r.get())\n",
    "\n",
    "#     return cleaned_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace859a4-bc0d-4e2f-a177-aef154803f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     import pickle\n",
    "    \n",
    "#     with open( r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\cleaned_texts_subpart1.pkl', 'rb') as f:\n",
    "#         cleaned_sentences_tokens = pickle.load(f)\n",
    "\n",
    "#     print(1)\n",
    "    \n",
    "#     second_cleaned_result = parallel_cleaning(\n",
    "#         cleaned_sentences_tokens,\n",
    "#         batch_size=100000,\n",
    "#         chunk_size=1000,\n",
    "#         max_workers=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e989b78-df0d-4caf-baa5-2c27cccb4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜\n",
    "# import pickle\n",
    "# with open( r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\second_cleaned_result_subpart1.pkl', 'wb') as f:\n",
    "#     pickle.dump(second_cleaned_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02a5444-47e4-4a71-ac9f-5f9eed359b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33376038"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(second_cleaned_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fad27a-a5cd-4b3c-807b-ee49098ce6cd",
   "metadata": {},
   "source": [
    "#### æŠŠæ¸…æ´—åçš„pklæ–‡ä»¶è½¬æ¢æˆå­—ç¬¦ä¸²å¹¶åˆå¹¶åˆ°ä¸€ä¸ªå¤§txtä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06be59ff-5b9e-4c09-afb2-bc6cf611e420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized sentences from D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\cleaned_texts_subpart5.pkl ...\n",
      "Converting tokenized sentences to strings ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28400169/28400169 [00:48<00:00, 587570.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\cleaned_texts_subpart5_str.txt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r\"D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\cleaned_texts_subpart5.pkl\"\n",
    "output_path = input_path.replace(\".pkl\", \"_str.txt\")\n",
    "\n",
    "print(f\"Loading tokenized sentences from {input_path} ...\")\n",
    "with open(input_path, \"rb\") as f:\n",
    "    sentences_tokenized = pickle.load(f)\n",
    "\n",
    "print(f\"Converting tokenized sentences to strings ...\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for sent in tqdm(sentences_tokenized):\n",
    "        sentence = \" \".join(sent)\n",
    "        f_out.write(sentence + \"\\n\")\n",
    "\n",
    "print(f\"Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebcb1b7-8fce-44f1-bd59-a3453723f5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\cleaned_texts_subpart1_str.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending cleaned_texts_subpart1_str.txt: 33376038it [00:55, 600420.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\cleaned_texts_subpart2_str.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending cleaned_texts_subpart2_str.txt: 35382251it [00:57, 615659.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\cleaned_texts_subpart3_str.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending cleaned_texts_subpart3_str.txt: 32427779it [00:51, 628252.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\cleaned_texts_subpart4_str.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending cleaned_texts_subpart4_str.txt: 29917349it [00:47, 634031.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\cleaned_texts_subpart5_str.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending cleaned_texts_subpart5_str.txt: 28400169it [00:43, 649856.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… åˆå¹¶å®Œæˆï¼Œå…±è®¡ 159,503,586 æ¡å¥å­ï¼Œä¿å­˜åˆ°ï¼šD:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\merged_cleaned_texts_str.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# è®¾ç½®æ–‡ä»¶å¤¹è·¯å¾„\n",
    "folder_path = r\"D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\"\n",
    "\n",
    "# åŒ¹é…æ‰€æœ‰ *_str.txt æ–‡ä»¶\n",
    "file_pattern = os.path.join(folder_path, \"cleaned_texts_subpart*_str.txt\")\n",
    "txt_files = sorted(glob(file_pattern))  # ç¡®ä¿é¡ºåºä¸€è‡´\n",
    "\n",
    "# è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "output_path = os.path.join(folder_path, \"merged_cleaned_texts_str.txt\")\n",
    "\n",
    "# åˆå¹¶æ–‡ä»¶\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    total_lines = 0\n",
    "    for file in txt_files:\n",
    "        print(f\"Processing {file} ...\")\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f_in:\n",
    "            for line in tqdm(f_in, desc=f\"Appending {os.path.basename(file)}\"):\n",
    "                f_out.write(line)\n",
    "                total_lines += 1\n",
    "\n",
    "print(f\"\\nâœ… åˆå¹¶å®Œæˆï¼Œå…±è®¡ {total_lines:,} æ¡å¥å­ï¼Œä¿å­˜åˆ°ï¼š{output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79ba2e-ada8-4dc1-ba9f-70bf607f7f7c",
   "metadata": {},
   "source": [
    "#### æŠŠä¸¤ä¸ªmerged_cleaned_texts_txtåˆå¹¶åˆ°ä¸€èµ·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96a4420-b7ad-40fc-9d4b-e2b21355b701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "åˆå¹¶è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292198355/292198355 [08:04<00:00, 602514.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… åˆå¹¶å®Œæˆï¼Œä¿å­˜è‡³ï¼šD:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\all_cleaned_txts_str.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å®šä¹‰æ–‡ä»¶å¤¹è·¯å¾„\n",
    "folder_path = r\"D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\"\n",
    "\n",
    "# å¾…åˆå¹¶çš„æ–‡ä»¶å\n",
    "file_names = [\"merged_cleaned_texts_str_1.txt\", \"merged_cleaned_texts_str_2.txt\"]\n",
    "\n",
    "# è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "output_path = os.path.join(folder_path, \"all_cleaned_txts_str.txt\")\n",
    "\n",
    "# è·å–æ¯ä¸ªæ–‡ä»¶çš„è¡Œæ•°ï¼ˆç”¨äºè¿›åº¦æ¡æ€»æ•°ï¼‰\n",
    "line_counts = []\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(folder_path, fname), 'r', encoding='utf-8') as f:\n",
    "        line_counts.append(sum(1 for _ in f))\n",
    "total_lines = sum(line_counts)\n",
    "\n",
    "# åˆå¹¶æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦æ¡\n",
    "with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "    with tqdm(total=total_lines, desc=\"åˆå¹¶è¿›åº¦\") as pbar:\n",
    "        for fname in file_names:\n",
    "            file_path = os.path.join(folder_path, fname)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "                    pbar.update(1)\n",
    "\n",
    "print(f\"\\nâœ… åˆå¹¶å®Œæˆï¼Œä¿å­˜è‡³ï¼š{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee7be9-1958-4c18-889c-a9d91b42525b",
   "metadata": {},
   "source": [
    "### æ£€æµ‹å¸¸ç”¨çŸ­è¯­å¹¶ç”¨\"-\"è¿æ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7188eea-19de-4a13-8130-9e56e5cd4a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phrases: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292198355/292198355 [2:20:49<00:00, 34581.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\all_cleaned_txts_str.txt'\n",
    "\n",
    "def sentence_generator(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield line.split()\n",
    "\n",
    "# å…ˆç»Ÿè®¡æ–‡ä»¶æ€»è¡Œæ•°ï¼Œæ–¹ä¾¿ tqdm æ˜¾ç¤ºè¿›åº¦\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "# é‡æ–°æ„é€  generator\n",
    "raw_sentences = sentence_generator(input_path)\n",
    "\n",
    "# tqdm åŒ…è£… generatorï¼Œæ˜¾ç¤ºè®­ç»ƒè¿›åº¦\n",
    "sentence_iter = tqdm(raw_sentences, total=total_lines, desc='Training Phrases')\n",
    "\n",
    "# è®­ç»ƒ Phrases æ¨¡å‹ï¼Œå‚æ•°æŒ‰ä½ ä¹‹å‰å®šçš„\n",
    "phr = Phrases(sentence_iter, min_count=1000, threshold=10, delimiter='_')\n",
    "\n",
    "# è½¬æ¢ä¸ºè½»é‡åŒ–æ¨¡å‹ï¼Œæ–¹ä¾¿åç»­å¿«é€Ÿè°ƒç”¨\n",
    "bigram = Phraser(phr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdea668f-8b30-43e0-a910-729bc02f3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹\n",
    "bigram.save('bigram_phraser_1000_10.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847bbdf7-1355-440f-9c21-3ee925605896",
   "metadata": {},
   "source": [
    "#### æ£€éªŒmin_countå’Œthresholdå‚æ•°çš„è®­ç»ƒæ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbad9188-b77a-4b4f-b1e7-1f351d0cd10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥è®­ç»ƒæ¨¡å‹\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "bigram = Phraser.load('bigram_phraser_1000_10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e32e0ae-1989-4471-afbe-41348c353768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['his', 'performance', 'leaps', 'disap_pointment', 'the', 'rest']\n"
     ]
    }
   ],
   "source": [
    "# ç¤ºä¾‹å¥å­\n",
    "sentence = ['his', 'performance', 'leaps', 'disap', 'pointment', 'the', 'rest']\n",
    "\n",
    "# åº”ç”¨ bigram æ¨¡å‹\n",
    "transformed = bigram[sentence]\n",
    "\n",
    "print(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe7ef24-ff86-42fb-9fe3-363e4fb1c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "score = bigram.phrasegrams.get(('law', 'abiding'))\n",
    "print(score)  # None å°±è¯´æ˜å®ƒä¸åœ¨çŸ­è¯­è¡¨ä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22b01a01-1fc5-45be-b39d-ec6dcee2d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n",
      "del_egate 11.086396448227925\n",
      "leaps_bounds 11.08544452356236\n",
      "solitude_timidity 11.079304953795473\n",
      "syrup_figs 11.065966081370123\n",
      "peo_pie 11.065482626108857\n",
      "evi_dence 11.050758633368071\n",
      "disap_pointment 11.039497114096957\n",
      "hoods_sarsaparilia 11.036471801416601\n",
      "anthra_cite 11.018956112120838\n",
      "indig_nant 11.01342443010717\n",
      "oils_varnishes 11.01089138316523\n",
      "millard_fillmore 11.006458872281518\n",
      "piano_fortes 10.991495506254747\n",
      "terri_tory 10.99148627136611\n",
      "archbishop_corrigan 10.9910904544091\n",
      "res_ervation 10.989627319148866\n",
      "para_graphs 10.982367307485687\n",
      "hos_pltal 10.958151023419816\n",
      "mis_sionary 10.954924312022676\n",
      "plum_pudding 10.946143735450848\n",
      "furni_ture 10.939312739140375\n",
      "eau_tiously 10.928432962112026\n",
      "degs_mins 10.9262276014228\n",
      "guardian_litem 10.919218449593146\n",
      "alcoholic_stimulants 10.918949555944755\n",
      "ambassa_dor 10.905689733222736\n",
      "asso_ciate 10.904944439281117\n",
      "cam_palgn 10.899509213227581\n",
      "shah_persia 10.898064010833115\n",
      "phe_nix 10.88896966635347\n",
      "cows_heifers 10.865063866981158\n",
      "presiden_tial 10.84734725099995\n",
      "conscientious_scruples 10.84441623576072\n",
      "deliv_cred 10.843060655784742\n",
      "angostura_bitters 10.834994242656892\n",
      "meas_ured 10.827989224928764\n",
      "clump_bushes 10.804764315329924\n",
      "doans_regulets 10.804114429225738\n",
      "shoops_restorative 10.80298750004823\n",
      "cen_turies 10.79316323934108\n",
      "eff_fective 10.78736503496133\n",
      "mani_fested 10.768813940479932\n",
      "cen_tral 10.76792770226878\n",
      "armored_cruiser 10.756364821328514\n",
      "gus_tody 10.751702907374156\n",
      "emerald_isle 10.751023847019606\n",
      "defaulting_purchaser 10.745285641679523\n",
      "rub_bish 10.737189398173173\n",
      "homo_lulu 10.735133965435793\n",
      "analytical_chemist 10.731548005980407\n",
      "corned_beef 10.728781849099548\n",
      "rep_resented 10.715375188997527\n",
      "knotted_fringe 10.708964028605353\n",
      "ayes_nays 10.707131320580993\n",
      "obituary_notices 10.694919120130544\n",
      "dilatory_motions 10.694605028538316\n",
      "bull_letin 10.683788073085305\n",
      "masted_schooner 10.682639948406086\n",
      "emulsion_codliver 10.68156979899434\n",
      "demo_gratis 10.679866755458645\n",
      "citi_zen 10.66795190602304\n",
      "coke_ovens 10.667178497703716\n",
      "rob_bery 10.6627644196759\n",
      "spirituous_intoxicating 10.65840441994927\n",
      "sur_roundings 10.657022746701596\n",
      "bach_elors 10.645914662171053\n",
      "rap_ids 10.640013574634324\n",
      "las_vegas 10.634385266096219\n",
      "chan_nels 10.618195190065002\n",
      "automo_bile 10.593505427502748\n",
      "trans_vaal 10.587841227323572\n",
      "heirs_devisees 10.587514695021662\n",
      "hoosac_tunnel 10.561609908495925\n",
      "jus_tices 10.552726714417085\n",
      "impor_tance 10.549656737021381\n",
      "listened_attentively 10.53986852264003\n",
      "tenn_perance 10.539698144546735\n",
      "conf_dence 10.536507645038297\n",
      "amer_lean 10.524789690291923\n",
      "rio_janero 10.519895672376194\n",
      "gath_cred 10.504592927896082\n",
      "cap_ital 10.48025124066124\n",
      "nom_inee 10.471095392739102\n",
      "claiming_versely 10.46536889850949\n",
      "signor_crispi 10.458412675461876\n",
      "pros_porous 10.454717640246097\n",
      "cab_inet 10.44979572817804\n",
      "tele_phone 10.440099415828644\n",
      "sigma_lures 10.43459600475989\n",
      "belle_fourche 10.426504437568838\n",
      "physi_elan 10.425218837351192\n",
      "exalted_ruler 10.422288054233146\n",
      "tele_gram 10.418635183563763\n",
      "claiming_adversely 10.41133108330847\n",
      "monte_vista 10.397413711577466\n",
      "furred_tongue 10.377570528373427\n",
      "wee_sma 10.360307700523038\n",
      "ian_guage 10.356546562051655\n",
      "toxicating_liquors 10.351467849226719\n",
      "cap_itol 10.335581310142713\n",
      "var_ious 10.332141394548573\n",
      "robley_evans 10.33103702671492\n",
      "oat_groats 10.330623347342039\n",
      "expert_enced 10.319652375106411\n",
      "peremptory_challenges 10.311440625411763\n",
      "hospi_tai 10.293952575274883\n",
      "kel_logg 10.289824870258883\n",
      "philip_pines 10.279326547032449\n",
      "clayton_bulwer 10.279076368302672\n",
      "maria_teresa 10.268320509868877\n",
      "prevention_cruelty 10.267088300364268\n",
      "superinten_dent 10.259556307405358\n",
      "vio_lence 10.258314168763492\n",
      "bone_sinew 10.253608201618084\n",
      "dan_gerously 10.252735821540714\n",
      "cali_forna 10.245171412598852\n",
      "robinson_crusoe 10.238864880639948\n",
      "pub_lication 10.237002486515516\n",
      "steering_gear 10.227766726228966\n",
      "asthma_bronchitis 10.196553756244366\n",
      "fif_teen 10.190797543552362\n",
      "lam_caster 10.190315573621675\n",
      "specta_tors 10.18258608824983\n",
      "dispels_colds 10.175300114248342\n",
      "nuptial_knot 10.174172592649473\n",
      "births_marriages 10.156416997973048\n",
      "reg_ulate 10.156092106862838\n",
      "chamberlains_collie 10.131605374410084\n",
      "carte_blanche 10.125491199085257\n",
      "conner_cial 10.106501720622193\n",
      "salts_faction 10.09670417337914\n",
      "congre_gation 10.090942033023014\n",
      "sandy_hook 10.082456449302455\n",
      "muriatic_acid 10.063877344970434\n",
      "laxative_bromoquinine 10.062119496054404\n",
      "oaken_bucket 10.055737520590085\n",
      "regis_trar 10.039767115122617\n",
      "sem_ator 10.013905776526762\n",
      "proba_bility 10.007952451684353\n",
      "meas_ure 10.000196886543694\n"
     ]
    }
   ],
   "source": [
    "print(len(bigram.phrasegrams))  # æ‰“å°æ¨¡å‹ä¸­è¯†åˆ«å‡ºçš„çŸ­è¯­æ•°é‡\n",
    "for phrase, score in sorted(bigram.phrasegrams.items(), key=lambda x: -x[1])[1844:]:\n",
    "    print(phrase, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3237eb-3d18-46e9-9fec-e4fa2e64dec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting bigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292198355/292198355 [48:13<00:00, 100968.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united states å‡ºç°æ¬¡æ•°: 4746255\n",
      "black people å‡ºç°æ¬¡æ•°: 2870\n",
      "white people å‡ºç°æ¬¡æ•°: 35741\n",
      "law abiding å‡ºç°æ¬¡æ•°: 13232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\all_cleaned_txts_str.txt'\n",
    "\n",
    "# å…ˆè·å–æ€»è¡Œæ•°ï¼Œç”¨äº tqdm æ˜¾ç¤ºè¿›åº¦æ¡\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "# åˆå§‹åŒ–è®¡æ•°å™¨\n",
    "united_states_count = 0\n",
    "black_people_count = 0\n",
    "white_people_count = 0\n",
    "law_abiding_count = 0\n",
    "\n",
    "# å¼€å§‹ç»Ÿè®¡\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, total=total_lines, desc='Counting bigrams'):\n",
    "        tokens = line.strip().split()\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i+1])\n",
    "            if pair == ('united', 'states'):\n",
    "                united_states_count += 1\n",
    "            elif pair == ('black', 'people'):\n",
    "                black_people_count += 1\n",
    "            elif pair == ('white', 'people'):\n",
    "                white_people_count += 1\n",
    "            elif pair == ('law', 'abiding'):\n",
    "                law_abiding_count += 1\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print('united states å‡ºç°æ¬¡æ•°:', united_states_count)\n",
    "print('black people å‡ºç°æ¬¡æ•°:', black_people_count)\n",
    "print('white people å‡ºç°æ¬¡æ•°:', white_people_count)\n",
    "print('law abiding å‡ºç°æ¬¡æ•°:', law_abiding_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dec903-2f24-40e8-88cb-1cc2dd6dd904",
   "metadata": {},
   "source": [
    "#### æ›¿æ¢æ–‡æœ¬ä¸­çš„å•è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a976df-ee0a-47e5-95bd-10f12960c031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292198355/292198355 [4:22:58<00:00, 18518.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜è‡³ï¼š D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\all_cleaned_txts_str_processed.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\all_cleaned_txts_str.txt'\n",
    "output_path = r'D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\all_cleaned_txts_str_processed.txt'\n",
    "\n",
    "# åŠ è½½è®­ç»ƒå¥½çš„bigramæ¨¡å‹\n",
    "bigram = Phraser.load('bigram_phraser_1000_10.pkl')\n",
    "\n",
    "# æ‰‹åŠ¨æŒ‡å®šéœ€è¦å¼ºåˆ¶åˆå¹¶çš„çŸ­è¯­ï¼ˆç”¨tupleå½¢å¼ï¼‰\n",
    "custom_phrases = [\n",
    "    ('law', 'abiding'),\n",
    "    ('black', 'people'),\n",
    "    ('white', 'people'),\n",
    "    ('lacking', 'ambition'),\n",
    "    ('welfare', 'dependent'),\n",
    "    ('very', 'religious'),\n",
    "    ('drug', 'abusers'),\n",
    "    ('physically', 'dirty'),\n",
    "    ('bad', 'dancers')]\n",
    "\n",
    "def manual_phrase_replace(tokens, phrase_list):\n",
    "    i = 0\n",
    "    result = []\n",
    "    while i < len(tokens):\n",
    "        replaced = False\n",
    "        for phrase in phrase_list:\n",
    "            length = len(phrase)\n",
    "            # åˆ¤æ–­å½“å‰ä½ç½®tokensæ˜¯å¦åŒ¹é…phrase\n",
    "            if tuple(tokens[i:i+length]) == phrase:\n",
    "                result.append('_'.join(phrase))  # åˆå¹¶çŸ­è¯­\n",
    "                i += length\n",
    "                replaced = True\n",
    "                break\n",
    "        if not replaced:\n",
    "            result.append(tokens[i])\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "# å…ˆç»Ÿè®¡æ€»è¡Œæ•°ï¼Œæ–¹ä¾¿tqdmæ˜¾ç¤ºè¿›åº¦\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as fin, \\\n",
    "     open(output_path, 'w', encoding='utf-8') as fout:\n",
    "    for line in tqdm(fin, total=total_lines, desc='Processing lines'):\n",
    "        tokens = line.strip().split()\n",
    "        # å…ˆç”¨bigramæ¨¡å‹æ›¿æ¢è‡ªåŠ¨è¯†åˆ«çš„çŸ­è¯­\n",
    "        bigram_tokens = bigram[tokens]\n",
    "        # å†ç”¨æ‰‹åŠ¨çŸ­è¯­æ›¿æ¢ï¼Œç¡®ä¿è‡ªå®šä¹‰çŸ­è¯­è¢«åˆå¹¶\n",
    "        final_tokens = manual_phrase_replace(bigram_tokens, custom_phrases)\n",
    "        # å†™å…¥æ–°æ–‡ä»¶\n",
    "        fout.write(' '.join(final_tokens) + '\\n')\n",
    "\n",
    "print('å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜è‡³ï¼š', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cc256-ad8b-4e2e-926a-1adcd10f920c",
   "metadata": {},
   "source": [
    "### è®­ç»ƒè¯å‘é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f484a-910c-4c27-bb1b-3628b105a446",
   "metadata": {},
   "source": [
    "#### è·‘è¯å‘é‡å‰å‚æ•°è®¾ç½®å‚è€ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f662e117-0224-469d-aef9-a298841d2c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ç»Ÿè®¡è¯é¢‘ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292198355/292198355 [29:02<00:00, 167672.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count=5 -> ä¿ç•™è¯æ•° â‰ˆ 6389908\n",
      "min_count=10 -> ä¿ç•™è¯æ•° â‰ˆ 3395622\n",
      "min_count=50 -> ä¿ç•™è¯æ•° â‰ˆ 877343\n",
      "min_count=100 -> ä¿ç•™è¯æ•° â‰ˆ 499004\n",
      "min_count=200 -> ä¿ç•™è¯æ•° â‰ˆ 288258\n",
      "min_count=300 -> ä¿ç•™è¯æ•° â‰ˆ 211961\n",
      "min_count=500 -> ä¿ç•™è¯æ•° â‰ˆ 145749\n",
      "min_count=1000 -> ä¿ç•™è¯æ•° â‰ˆ 91143\n",
      "min_count=2000 -> ä¿ç•™è¯æ•° â‰ˆ 58937\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r\"D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\all_cleaned_txts_str_processed.txt\"\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "# å…ˆè·å–æ–‡ä»¶æ€»è¡Œæ•°ï¼ˆç”¨æ¥åˆå§‹åŒ– tqdmï¼‰\n",
    "def count_lines(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for i, _ in enumerate(f, 1):\n",
    "            pass\n",
    "    return i\n",
    "\n",
    "total_lines = count_lines(input_path)\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, total=total_lines, desc=\"ç»Ÿè®¡è¯é¢‘ä¸­\"):\n",
    "        tokens = line.strip().split()\n",
    "        counter.update(tokens)\n",
    "\n",
    "# ç»Ÿè®¡å„ min_count ä¸‹çš„è¯æ•°\n",
    "for threshold in [5, 10, 50, 100, 200, 300, 500, 1000, 2000]:\n",
    "    retained = sum(1 for count in counter.values() if count >= threshold)\n",
    "    print(f\"min_count={threshold} -> ä¿ç•™è¯æ•° â‰ˆ {retained}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87d8834-32c3-4af8-8d29-1e1ccd57ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“š Training Word2Vec: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [23:07:05<00:00, 8322.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Word2Vec æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¸è¯å‘é‡å·²ä¿å­˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "# è‡ªå®šä¹‰å›è°ƒç±»ï¼šè®­ç»ƒè¿›åº¦å¯è§†åŒ–\n",
    "class TQDMProgressBar(CallbackAny2Vec):\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.pbar = tqdm(total=epochs, desc=\"ğŸ“š Training Word2Vec\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def on_train_end(self, model):\n",
    "        self.pbar.close()\n",
    "\n",
    "\n",
    "# è®¾ç½®è¯­æ–™è·¯å¾„\n",
    "input_path = r\"D:\\zhenfeng zhou\\è‡ªç”±çš„ä»£ä»·\\æ‰©å±•ç§æ—ä¸»ä¹‰è¯å…¸\\full articles\\å…¨æ–‡\\æ•°æ®å¤ªå¤§åˆ†å¼€æ¸…æ´—\\all_cleaned_txts_str_processed.txt\"\n",
    "\n",
    "# æµå¼åŠ è½½è¯­æ–™ï¼ˆæ¯è¡Œä¸ºä¸€ä¸ªå¥å­ï¼Œç©ºæ ¼åˆ†è¯ï¼‰\n",
    "sentences = LineSentence(input_path)\n",
    "\n",
    "# è®¾ç½® epoch æ¬¡æ•°\n",
    "epochs = 10\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=300,\n",
    "    window=6,\n",
    "    negative=10,\n",
    "    min_count=500,\n",
    "    workers=32,\n",
    "    sg=1,\n",
    "    epochs=epochs,\n",
    "    compute_loss=True,\n",
    "    callbacks=[TQDMProgressBar(epochs)])\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "model.save(\"word2vec_large_300million.model\")\n",
    "model.wv.save_word2vec_format(\"word2vec_large_vectors_300million.txt\", binary=False)\n",
    "\n",
    "print(\"âœ… Word2Vec æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¸è¯å‘é‡å·²ä¿å­˜ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f77cf-e884-4a97-9ccc-182861080710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
