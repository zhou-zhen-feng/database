{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c91eaaf-365d-4916-b4ef-7dfbe3cc862c",
   "metadata": {},
   "source": [
    "### 全部语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525da5d6-3b4a-4bb4-914a-a411ffb333fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import json\n",
    "import traceback\n",
    "import threading\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f656fa93-f63c-4c0f-80a7-d70741683f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "df_1800_1910_part1 = pd.read_csv(r\"D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\1800_1910_american_story_part1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa2a60f-a35e-4b5f-bf2c-df1d69a43733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"By ORDER OF COURT,\\nnNE third part of piece Of Flats\\n~ Land, in Portland, in she county of Cumberland,\\nwith one third part of the wharf thereon Handing,\\nmonty called Weeks wharf.\\n\\n\\nAlto, another piece of Flats Land,\\nituae nearly oppoGte the houie of Mr.. Thomas Beck\\nPortland sforcad. the fame being the fare of Flats\\nbelonging to piece of land fold by THOMAS cunMIso\\nlate ct aid Portland, merchant deceaed, to Samuel\\nM Quincy.\\n\\n\\nALTO, large lot of Land fituate in\\nin King .frwt in the fame Portland. with dwelling\\nHoue ianeins thereon, aud the other buildings the\\nfame, being the dwelling houfe OF the fad Thomas,\\nexcept that part of faid lot and buildings which\\nft on to the fubcriber. widow o faid deeesied her\\ndover in her faid hulband's .eaate.\\n\\n\\nALTO, the remainder of faid lot lait\\nmentioned. and the buildings and sppurtenances fub-\\nje only to the life effA,e OF the fad tenant dover.\\nThe above being all the real elfate\\nwhereof the faid Thomas died feized and ponefed.\\nwill be fold on the day above mentioned, the pre-\\nmifes; or 70 much thereof a. wiz be fuf6cent pay\\nthe Jun debts Of the fad dtceafed, With incident1\\ncharges.\\n\\n\\nD&fss at Portl~rf. n. 1,ld\\nFsyrs1r,, .4~s D.~;ni 1900.\\n\\n\\nEleonora Cumming,\\nddssjsiarsty;x On fs;j Mafs.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1800_1910_part1[\"Article_body\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d212b06d-149f-4553-89d4-3a15b9993ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把列表中，每一个字典元素中的white_contexts和black_contexts合并成一个text,没有的话跳过\n",
    "# from tqdm import tqdm \n",
    "# merged_blackswhites_texts = []\n",
    "\n",
    "# for item in tqdm(data):\n",
    "#     # 合并 white_contexts 和 black_contexts\n",
    "#     contexts = item['white_contexts'] + item['black_contexts']\n",
    "#     if contexts:  # 如果不为空\n",
    "#         merged_text = ' '.join(contexts)  # 合并成一个字符串\n",
    "#         merged_blackswhites_texts.append(merged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93c573f-42c1-413a-92c3-ff49b7e1a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(merged_blackswhites_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245f227-021c-461f-992b-013860e9fdd2",
   "metadata": {},
   "source": [
    "### 筛选,60%以上是英文词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67466ca9-2fa0-4657-a9ff-8f08bed0d9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('state_union')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e577710-6ec1-44ff-8abd-c02a1ca514a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from match_function import process_chunk, init_worker\n",
    "\n",
    "def parallel_filter_texts_pipe(texts, batch_size=10000, chunk_size=1000, max_workers=32):\n",
    "    \"\"\"并行批量处理文本，返回筛选后的结果\"\"\"\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "    filtered_all = []\n",
    "\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start:start + batch_size]\n",
    "        chunks = [batch[i:i + chunk_size] for i in range(0, len(batch), chunk_size)]\n",
    "\n",
    "        with mp.Pool(processes=max_workers, initializer=init_worker) as pool:\n",
    "            results = [\n",
    "                pool.apply_async(process_chunk, args=(chunk, progress_queue))\n",
    "                for chunk in chunks\n",
    "            ]\n",
    "\n",
    "            with tqdm(total=len(batch), desc=f\"Filtering [{start}-{start + len(batch)}]\") as pbar:\n",
    "                for _ in range(len(batch)):\n",
    "                    progress_queue.get()\n",
    "                    pbar.update(1)\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        for r in results:\n",
    "            filtered_all.extend([res for res in r.get() if res is not None])\n",
    "\n",
    "    return filtered_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef26f6ee-da17-4a33-ab41-67b34f137819",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts = df_1800_1910_part1[\"Article_body\"].tolist()\n",
    "    \n",
    "    print(1)\n",
    "    \n",
    "    filtered_texts = parallel_filter_texts_pipe(\n",
    "        texts,\n",
    "        batch_size=100000,\n",
    "        chunk_size=10000,\n",
    "        max_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c23c79ea-2f45-461e-bf61-526634f8d8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29754890"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86193205-4948-44c4-bedd-6eddc531d585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"By ORDER OF COURT, nNE third part of piece Of Flats ~ Land, in Portland, in she county of Cumberland, with one third part of the wharf thereon Handing, monty called Weeks wharf.   Alto, another piece of Flats Land, ituae nearly oppoGte the houie of Mr.. Thomas Beck Portland sforcad. the fame being the fare of Flats belonging to piece of land fold by THOMAS cunMIso late ct aid Portland, merchant deceaed, to Samuel M Quincy.   ALTO, large lot of Land fituate in in King .frwt in the fame Portland. with dwelling Houe ianeins thereon, aud the other buildings the fame, being the dwelling houfe OF the fad Thomas, except that part of faid lot and buildings which ft on to the fubcriber. widow o faid deeesied her dover in her faid hulband's .eaate.   ALTO, the remainder of faid lot lait mentioned. and the buildings and sppurtenances fubje only to the life effA,e OF the fad tenant dover. The above being all the real elfate whereof the faid Thomas died feized and ponefed. will be fold on the day above mentioned, the premifes; or 70 much thereof a. wiz be fuf6cent pay the Jun debts Of the fad dtceafed, With incident1 charges.   D&fss at Portl~rf. n. 1,ld Fsyrs1r,, .4~s D.~;ni 1900.   Eleonora Cumming, ddssjsiarsty;x On fs;j Mafs.\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_texts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "936a53b8-4f05-4b0e-85e3-c845a4350489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# 保存\n",
    "with open( r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\filtered_texts_1.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66756ee9-f9eb-42c0-bf0f-75f10b588635",
   "metadata": {},
   "source": [
    "### 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730fa463-ddd5-4986-866e-1fd7171f76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据太大 分开清洗\n",
    "import pickle\n",
    "\n",
    "# 读取原始数据\n",
    "with open(r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\filtered_texts_1.pkl', 'rb') as f:\n",
    "    texts = pickle.load(f)\n",
    "\n",
    "# 计算每一份的长度\n",
    "total_len = len(texts)\n",
    "num_parts = 5\n",
    "part_len = total_len // num_parts\n",
    "\n",
    "# 分五份（最后一份包含剩余）\n",
    "texts_part1 = texts[0:part_len]\n",
    "texts_part2 = texts[part_len:2*part_len]\n",
    "texts_part3 = texts[2*part_len:3*part_len]\n",
    "texts_part4 = texts[3*part_len:4*part_len]\n",
    "texts_part5 = texts[4*part_len:]\n",
    "\n",
    "# 保存五份\n",
    "base_path = r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗'\n",
    "\n",
    "with open(base_path + r'\\filtered_texts_1_subpart1.pkl', 'wb') as f:\n",
    "    pickle.dump(texts_part1, f)\n",
    "\n",
    "with open(base_path + r'\\filtered_texts_1_subpart2.pkl', 'wb') as f:\n",
    "    pickle.dump(texts_part2, f)\n",
    "\n",
    "with open(base_path + r'\\filtered_texts_1_subpart3.pkl', 'wb') as f:\n",
    "    pickle.dump(texts_part3, f)\n",
    "\n",
    "with open(base_path + r'\\filtered_texts_1_subpart4.pkl', 'wb') as f:\n",
    "    pickle.dump(texts_part4, f)\n",
    "\n",
    "with open(base_path + r'\\filtered_texts_1_subpart5.pkl', 'wb') as f:\n",
    "    pickle.dump(texts_part5, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb873286-2623-4393-bfc8-d4ccee771bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_cleaner.py\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "import contractions\n",
    "from match_function import process_text_2,init_worker_2,is_valid_token\n",
    "\n",
    "def parallel_clean_texts(texts, batch_size=100000, chunk_size=1000, max_workers=32):\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "    finally_results = []\n",
    "\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start:start + batch_size]\n",
    "        chunks = [batch[i:i + chunk_size] for i in range(0, len(batch), chunk_size)]\n",
    "\n",
    "        with mp.Pool(processes=max_workers, initializer=init_worker_2) as pool:\n",
    "            async_results = [\n",
    "                pool.apply_async(process_text_2, args=(chunk, progress_queue))\n",
    "                for chunk in chunks]\n",
    "\n",
    "            # 显示子任务处理进度\n",
    "            with tqdm(total=len(batch), desc=f\"Filtering [{start}-{start + len(batch)}]\") as pbar:\n",
    "                for _ in range(len(batch)):\n",
    "                    progress_queue.get()\n",
    "                    pbar.update(1)\n",
    "\n",
    "            # ✅ 在 Pool 内获取结果，避免卡死\n",
    "            for r in tqdm(async_results, desc=\"Collecting batch results\"):\n",
    "                try:\n",
    "                    results = r.get(timeout=60)  # 加上 timeout 更稳\n",
    "                    if results:\n",
    "                        finally_results.extend(results)\n",
    "                except Exception as e:\n",
    "                    # print(f\"[WARNING] Skipped result due to error: {e}\")\n",
    "                    continue\n",
    "\n",
    "    return finally_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b2466e-70e4-40d2-948d-22581216c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # texts = df_1800_1910_select_2['Article_body'].tolist()\n",
    "    import pickle\n",
    "    with open(r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\filtered_texts_1_subpart5.pkl', 'rb') as f:\n",
    "       texts = pickle.load(f)\n",
    "\n",
    "    print(1)\n",
    "\n",
    "    cleaned_sentences_tokens = parallel_clean_texts(texts,batch_size=100000, chunk_size=1000, max_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e4af48-b3e0-422a-8198-07da70b8a839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28400169"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_sentences_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8262e711-5bfb-4a72-ad05-48d0f0e87661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "import pickle\n",
    "with open( r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\cleaned_texts_subpart5.pkl', 'wb') as f:\n",
    "    \n",
    "    pickle.dump(cleaned_sentences_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be534e-6443-4232-a4e3-bd762665efe0",
   "metadata": {},
   "source": [
    "#### 针对文本列表的二次清洗 (只用一次）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c152bf-6256-4641-bccd-53a7580e98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# from match_function import clean_chunk,is_valid_token_1,clean_sentence\n",
    "# import re\n",
    "\n",
    "# def parallel_cleaning(all_sentences, batch_size=100000, chunk_size=1000, max_workers=30):\n",
    "#     manager = mp.Manager()\n",
    "#     progress_queue = manager.Queue()\n",
    "#     cleaned_all = []\n",
    "\n",
    "#     for batch_start in range(0, len(all_sentences), batch_size):\n",
    "#         batch = all_sentences[batch_start:batch_start + batch_size]\n",
    "#         chunks = [batch[i:i + chunk_size] for i in range(0, len(batch), chunk_size)]\n",
    "\n",
    "#         with mp.Pool(processes=max_workers) as pool:\n",
    "#             results = []\n",
    "#             for chunk in chunks:\n",
    "#                 res = pool.apply_async(clean_chunk, args=(chunk, progress_queue))\n",
    "#                 results.append(res)\n",
    "\n",
    "#             # Real-time progress bar per sentence\n",
    "#             with tqdm(total=len(batch), desc=f\"Filtering [{batch_start}-{batch_start + len(batch)}]\", unit=\"sents\") as pbar:\n",
    "#                 for _ in range(len(batch)):\n",
    "#                     progress_queue.get()\n",
    "#                     pbar.update(1)\n",
    "\n",
    "#             # Collect results\n",
    "#             for r in results:\n",
    "#                 cleaned_all.extend(r.get())\n",
    "\n",
    "#     return cleaned_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace859a4-bc0d-4e2f-a177-aef154803f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     import pickle\n",
    "    \n",
    "#     with open( r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\cleaned_texts_subpart1.pkl', 'rb') as f:\n",
    "#         cleaned_sentences_tokens = pickle.load(f)\n",
    "\n",
    "#     print(1)\n",
    "    \n",
    "#     second_cleaned_result = parallel_cleaning(\n",
    "#         cleaned_sentences_tokens,\n",
    "#         batch_size=100000,\n",
    "#         chunk_size=1000,\n",
    "#         max_workers=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e989b78-df0d-4caf-baa5-2c27cccb4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "# import pickle\n",
    "# with open( r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\second_cleaned_result_subpart1.pkl', 'wb') as f:\n",
    "#     pickle.dump(second_cleaned_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02a5444-47e4-4a71-ac9f-5f9eed359b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33376038"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(second_cleaned_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fad27a-a5cd-4b3c-807b-ee49098ce6cd",
   "metadata": {},
   "source": [
    "#### 把清洗后的pkl文件转换成字符串并合并到一个大txt中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06be59ff-5b9e-4c09-afb2-bc6cf611e420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized sentences from D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\cleaned_texts_subpart5.pkl ...\n",
      "Converting tokenized sentences to strings ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 28400169/28400169 [00:48<00:00, 587570.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\cleaned_texts_subpart5_str.txt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r\"D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\cleaned_texts_subpart5.pkl\"\n",
    "output_path = input_path.replace(\".pkl\", \"_str.txt\")\n",
    "\n",
    "print(f\"Loading tokenized sentences from {input_path} ...\")\n",
    "with open(input_path, \"rb\") as f:\n",
    "    sentences_tokenized = pickle.load(f)\n",
    "\n",
    "print(f\"Converting tokenized sentences to strings ...\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for sent in tqdm(sentences_tokenized):\n",
    "        sentence = \" \".join(sent)\n",
    "        f_out.write(sentence + \"\\n\")\n",
    "\n",
    "print(f\"Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebcb1b7-8fce-44f1-bd59-a3453723f5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\cleaned_texts_subpart1_str.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending cleaned_texts_subpart1_str.txt: 33376038it [00:55, 600420.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\cleaned_texts_subpart2_str.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending cleaned_texts_subpart2_str.txt: 35382251it [00:57, 615659.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\cleaned_texts_subpart3_str.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending cleaned_texts_subpart3_str.txt: 32427779it [00:51, 628252.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\cleaned_texts_subpart4_str.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending cleaned_texts_subpart4_str.txt: 29917349it [00:47, 634031.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\cleaned_texts_subpart5_str.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending cleaned_texts_subpart5_str.txt: 28400169it [00:43, 649856.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 合并完成，共计 159,503,586 条句子，保存到：D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\merged_cleaned_texts_str.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置文件夹路径\n",
    "folder_path = r\"D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\"\n",
    "\n",
    "# 匹配所有 *_str.txt 文件\n",
    "file_pattern = os.path.join(folder_path, \"cleaned_texts_subpart*_str.txt\")\n",
    "txt_files = sorted(glob(file_pattern))  # 确保顺序一致\n",
    "\n",
    "# 输出文件路径\n",
    "output_path = os.path.join(folder_path, \"merged_cleaned_texts_str.txt\")\n",
    "\n",
    "# 合并文件\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    total_lines = 0\n",
    "    for file in txt_files:\n",
    "        print(f\"Processing {file} ...\")\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f_in:\n",
    "            for line in tqdm(f_in, desc=f\"Appending {os.path.basename(file)}\"):\n",
    "                f_out.write(line)\n",
    "                total_lines += 1\n",
    "\n",
    "print(f\"\\n✅ 合并完成，共计 {total_lines:,} 条句子，保存到：{output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79ba2e-ada8-4dc1-ba9f-70bf607f7f7c",
   "metadata": {},
   "source": [
    "#### 把两个merged_cleaned_texts_txt合并到一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96a4420-b7ad-40fc-9d4b-e2b21355b701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "合并进度: 100%|██████████████████████████████████████████████████████| 292198355/292198355 [08:04<00:00, 602514.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 合并完成，保存至：D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\all_cleaned_txts_str.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义文件夹路径\n",
    "folder_path = r\"D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\"\n",
    "\n",
    "# 待合并的文件名\n",
    "file_names = [\"merged_cleaned_texts_str_1.txt\", \"merged_cleaned_texts_str_2.txt\"]\n",
    "\n",
    "# 输出文件路径\n",
    "output_path = os.path.join(folder_path, \"all_cleaned_txts_str.txt\")\n",
    "\n",
    "# 获取每个文件的行数（用于进度条总数）\n",
    "line_counts = []\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(folder_path, fname), 'r', encoding='utf-8') as f:\n",
    "        line_counts.append(sum(1 for _ in f))\n",
    "total_lines = sum(line_counts)\n",
    "\n",
    "# 合并文件并显示进度条\n",
    "with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "    with tqdm(total=total_lines, desc=\"合并进度\") as pbar:\n",
    "        for fname in file_names:\n",
    "            file_path = os.path.join(folder_path, fname)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "                    pbar.update(1)\n",
    "\n",
    "print(f\"\\n✅ 合并完成，保存至：{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee7be9-1958-4c18-889c-a9d91b42525b",
   "metadata": {},
   "source": [
    "### 检测常用短语并用\"-\"连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7188eea-19de-4a13-8130-9e56e5cd4a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phrases: 100%|█████████████████████████████████████████████| 292198355/292198355 [2:20:49<00:00, 34581.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\all_cleaned_txts_str.txt'\n",
    "\n",
    "def sentence_generator(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield line.split()\n",
    "\n",
    "# 先统计文件总行数，方便 tqdm 显示进度\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "# 重新构造 generator\n",
    "raw_sentences = sentence_generator(input_path)\n",
    "\n",
    "# tqdm 包装 generator，显示训练进度\n",
    "sentence_iter = tqdm(raw_sentences, total=total_lines, desc='Training Phrases')\n",
    "\n",
    "# 训练 Phrases 模型，参数按你之前定的\n",
    "phr = Phrases(sentence_iter, min_count=1000, threshold=10, delimiter='_')\n",
    "\n",
    "# 转换为轻量化模型，方便后续快速调用\n",
    "bigram = Phraser(phr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdea668f-8b30-43e0-a910-729bc02f3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "bigram.save('bigram_phraser_1000_10.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847bbdf7-1355-440f-9c21-3ee925605896",
   "metadata": {},
   "source": [
    "#### 检验min_count和threshold参数的训练效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbad9188-b77a-4b4f-b1e7-1f351d0cd10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入训练模型\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "bigram = Phraser.load('bigram_phraser_1000_10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e32e0ae-1989-4471-afbe-41348c353768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['his', 'performance', 'leaps', 'disap_pointment', 'the', 'rest']\n"
     ]
    }
   ],
   "source": [
    "# 示例句子\n",
    "sentence = ['his', 'performance', 'leaps', 'disap', 'pointment', 'the', 'rest']\n",
    "\n",
    "# 应用 bigram 模型\n",
    "transformed = bigram[sentence]\n",
    "\n",
    "print(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe7ef24-ff86-42fb-9fe3-363e4fb1c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "score = bigram.phrasegrams.get(('law', 'abiding'))\n",
    "print(score)  # None 就说明它不在短语表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22b01a01-1fc5-45be-b39d-ec6dcee2d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n",
      "del_egate 11.086396448227925\n",
      "leaps_bounds 11.08544452356236\n",
      "solitude_timidity 11.079304953795473\n",
      "syrup_figs 11.065966081370123\n",
      "peo_pie 11.065482626108857\n",
      "evi_dence 11.050758633368071\n",
      "disap_pointment 11.039497114096957\n",
      "hoods_sarsaparilia 11.036471801416601\n",
      "anthra_cite 11.018956112120838\n",
      "indig_nant 11.01342443010717\n",
      "oils_varnishes 11.01089138316523\n",
      "millard_fillmore 11.006458872281518\n",
      "piano_fortes 10.991495506254747\n",
      "terri_tory 10.99148627136611\n",
      "archbishop_corrigan 10.9910904544091\n",
      "res_ervation 10.989627319148866\n",
      "para_graphs 10.982367307485687\n",
      "hos_pltal 10.958151023419816\n",
      "mis_sionary 10.954924312022676\n",
      "plum_pudding 10.946143735450848\n",
      "furni_ture 10.939312739140375\n",
      "eau_tiously 10.928432962112026\n",
      "degs_mins 10.9262276014228\n",
      "guardian_litem 10.919218449593146\n",
      "alcoholic_stimulants 10.918949555944755\n",
      "ambassa_dor 10.905689733222736\n",
      "asso_ciate 10.904944439281117\n",
      "cam_palgn 10.899509213227581\n",
      "shah_persia 10.898064010833115\n",
      "phe_nix 10.88896966635347\n",
      "cows_heifers 10.865063866981158\n",
      "presiden_tial 10.84734725099995\n",
      "conscientious_scruples 10.84441623576072\n",
      "deliv_cred 10.843060655784742\n",
      "angostura_bitters 10.834994242656892\n",
      "meas_ured 10.827989224928764\n",
      "clump_bushes 10.804764315329924\n",
      "doans_regulets 10.804114429225738\n",
      "shoops_restorative 10.80298750004823\n",
      "cen_turies 10.79316323934108\n",
      "eff_fective 10.78736503496133\n",
      "mani_fested 10.768813940479932\n",
      "cen_tral 10.76792770226878\n",
      "armored_cruiser 10.756364821328514\n",
      "gus_tody 10.751702907374156\n",
      "emerald_isle 10.751023847019606\n",
      "defaulting_purchaser 10.745285641679523\n",
      "rub_bish 10.737189398173173\n",
      "homo_lulu 10.735133965435793\n",
      "analytical_chemist 10.731548005980407\n",
      "corned_beef 10.728781849099548\n",
      "rep_resented 10.715375188997527\n",
      "knotted_fringe 10.708964028605353\n",
      "ayes_nays 10.707131320580993\n",
      "obituary_notices 10.694919120130544\n",
      "dilatory_motions 10.694605028538316\n",
      "bull_letin 10.683788073085305\n",
      "masted_schooner 10.682639948406086\n",
      "emulsion_codliver 10.68156979899434\n",
      "demo_gratis 10.679866755458645\n",
      "citi_zen 10.66795190602304\n",
      "coke_ovens 10.667178497703716\n",
      "rob_bery 10.6627644196759\n",
      "spirituous_intoxicating 10.65840441994927\n",
      "sur_roundings 10.657022746701596\n",
      "bach_elors 10.645914662171053\n",
      "rap_ids 10.640013574634324\n",
      "las_vegas 10.634385266096219\n",
      "chan_nels 10.618195190065002\n",
      "automo_bile 10.593505427502748\n",
      "trans_vaal 10.587841227323572\n",
      "heirs_devisees 10.587514695021662\n",
      "hoosac_tunnel 10.561609908495925\n",
      "jus_tices 10.552726714417085\n",
      "impor_tance 10.549656737021381\n",
      "listened_attentively 10.53986852264003\n",
      "tenn_perance 10.539698144546735\n",
      "conf_dence 10.536507645038297\n",
      "amer_lean 10.524789690291923\n",
      "rio_janero 10.519895672376194\n",
      "gath_cred 10.504592927896082\n",
      "cap_ital 10.48025124066124\n",
      "nom_inee 10.471095392739102\n",
      "claiming_versely 10.46536889850949\n",
      "signor_crispi 10.458412675461876\n",
      "pros_porous 10.454717640246097\n",
      "cab_inet 10.44979572817804\n",
      "tele_phone 10.440099415828644\n",
      "sigma_lures 10.43459600475989\n",
      "belle_fourche 10.426504437568838\n",
      "physi_elan 10.425218837351192\n",
      "exalted_ruler 10.422288054233146\n",
      "tele_gram 10.418635183563763\n",
      "claiming_adversely 10.41133108330847\n",
      "monte_vista 10.397413711577466\n",
      "furred_tongue 10.377570528373427\n",
      "wee_sma 10.360307700523038\n",
      "ian_guage 10.356546562051655\n",
      "toxicating_liquors 10.351467849226719\n",
      "cap_itol 10.335581310142713\n",
      "var_ious 10.332141394548573\n",
      "robley_evans 10.33103702671492\n",
      "oat_groats 10.330623347342039\n",
      "expert_enced 10.319652375106411\n",
      "peremptory_challenges 10.311440625411763\n",
      "hospi_tai 10.293952575274883\n",
      "kel_logg 10.289824870258883\n",
      "philip_pines 10.279326547032449\n",
      "clayton_bulwer 10.279076368302672\n",
      "maria_teresa 10.268320509868877\n",
      "prevention_cruelty 10.267088300364268\n",
      "superinten_dent 10.259556307405358\n",
      "vio_lence 10.258314168763492\n",
      "bone_sinew 10.253608201618084\n",
      "dan_gerously 10.252735821540714\n",
      "cali_forna 10.245171412598852\n",
      "robinson_crusoe 10.238864880639948\n",
      "pub_lication 10.237002486515516\n",
      "steering_gear 10.227766726228966\n",
      "asthma_bronchitis 10.196553756244366\n",
      "fif_teen 10.190797543552362\n",
      "lam_caster 10.190315573621675\n",
      "specta_tors 10.18258608824983\n",
      "dispels_colds 10.175300114248342\n",
      "nuptial_knot 10.174172592649473\n",
      "births_marriages 10.156416997973048\n",
      "reg_ulate 10.156092106862838\n",
      "chamberlains_collie 10.131605374410084\n",
      "carte_blanche 10.125491199085257\n",
      "conner_cial 10.106501720622193\n",
      "salts_faction 10.09670417337914\n",
      "congre_gation 10.090942033023014\n",
      "sandy_hook 10.082456449302455\n",
      "muriatic_acid 10.063877344970434\n",
      "laxative_bromoquinine 10.062119496054404\n",
      "oaken_bucket 10.055737520590085\n",
      "regis_trar 10.039767115122617\n",
      "sem_ator 10.013905776526762\n",
      "proba_bility 10.007952451684353\n",
      "meas_ure 10.000196886543694\n"
     ]
    }
   ],
   "source": [
    "print(len(bigram.phrasegrams))  # 打印模型中识别出的短语数量\n",
    "for phrase, score in sorted(bigram.phrasegrams.items(), key=lambda x: -x[1])[1844:]:\n",
    "    print(phrase, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3237eb-3d18-46e9-9fec-e4fa2e64dec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting bigrams: 100%|██████████████████████████████████████████████| 292198355/292198355 [48:13<00:00, 100968.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united states 出现次数: 4746255\n",
      "black people 出现次数: 2870\n",
      "white people 出现次数: 35741\n",
      "law abiding 出现次数: 13232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\all_cleaned_txts_str.txt'\n",
    "\n",
    "# 先获取总行数，用于 tqdm 显示进度条\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "# 初始化计数器\n",
    "united_states_count = 0\n",
    "black_people_count = 0\n",
    "white_people_count = 0\n",
    "law_abiding_count = 0\n",
    "\n",
    "# 开始统计\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, total=total_lines, desc='Counting bigrams'):\n",
    "        tokens = line.strip().split()\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i+1])\n",
    "            if pair == ('united', 'states'):\n",
    "                united_states_count += 1\n",
    "            elif pair == ('black', 'people'):\n",
    "                black_people_count += 1\n",
    "            elif pair == ('white', 'people'):\n",
    "                white_people_count += 1\n",
    "            elif pair == ('law', 'abiding'):\n",
    "                law_abiding_count += 1\n",
    "\n",
    "# 输出结果\n",
    "print('united states 出现次数:', united_states_count)\n",
    "print('black people 出现次数:', black_people_count)\n",
    "print('white people 出现次数:', white_people_count)\n",
    "print('law abiding 出现次数:', law_abiding_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dec903-2f24-40e8-88cb-1cc2dd6dd904",
   "metadata": {},
   "source": [
    "#### 替换文本中的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a976df-ee0a-47e5-95bd-10f12960c031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|█████████████████████████████████████████████| 292198355/292198355 [4:22:58<00:00, 18518.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果保存至： D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\all_cleaned_txts_str_processed.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\all_cleaned_txts_str.txt'\n",
    "output_path = r'D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\all_cleaned_txts_str_processed.txt'\n",
    "\n",
    "# 加载训练好的bigram模型\n",
    "bigram = Phraser.load('bigram_phraser_1000_10.pkl')\n",
    "\n",
    "# 手动指定需要强制合并的短语（用tuple形式）\n",
    "custom_phrases = [\n",
    "    ('law', 'abiding'),\n",
    "    ('black', 'people'),\n",
    "    ('white', 'people'),\n",
    "    ('lacking', 'ambition'),\n",
    "    ('welfare', 'dependent'),\n",
    "    ('very', 'religious'),\n",
    "    ('drug', 'abusers'),\n",
    "    ('physically', 'dirty'),\n",
    "    ('bad', 'dancers')]\n",
    "\n",
    "def manual_phrase_replace(tokens, phrase_list):\n",
    "    i = 0\n",
    "    result = []\n",
    "    while i < len(tokens):\n",
    "        replaced = False\n",
    "        for phrase in phrase_list:\n",
    "            length = len(phrase)\n",
    "            # 判断当前位置tokens是否匹配phrase\n",
    "            if tuple(tokens[i:i+length]) == phrase:\n",
    "                result.append('_'.join(phrase))  # 合并短语\n",
    "                i += length\n",
    "                replaced = True\n",
    "                break\n",
    "        if not replaced:\n",
    "            result.append(tokens[i])\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "# 先统计总行数，方便tqdm显示进度\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as fin, \\\n",
    "     open(output_path, 'w', encoding='utf-8') as fout:\n",
    "    for line in tqdm(fin, total=total_lines, desc='Processing lines'):\n",
    "        tokens = line.strip().split()\n",
    "        # 先用bigram模型替换自动识别的短语\n",
    "        bigram_tokens = bigram[tokens]\n",
    "        # 再用手动短语替换，确保自定义短语被合并\n",
    "        final_tokens = manual_phrase_replace(bigram_tokens, custom_phrases)\n",
    "        # 写入新文件\n",
    "        fout.write(' '.join(final_tokens) + '\\n')\n",
    "\n",
    "print('处理完成，结果保存至：', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cc256-ad8b-4e2e-926a-1adcd10f920c",
   "metadata": {},
   "source": [
    "### 训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f484a-910c-4c27-bb1b-3628b105a446",
   "metadata": {},
   "source": [
    "#### 跑词向量前参数设置参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f662e117-0224-469d-aef9-a298841d2c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "统计词频中: 100%|████████████████████████████████████████████████████| 292198355/292198355 [29:02<00:00, 167672.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count=5 -> 保留词数 ≈ 6389908\n",
      "min_count=10 -> 保留词数 ≈ 3395622\n",
      "min_count=50 -> 保留词数 ≈ 877343\n",
      "min_count=100 -> 保留词数 ≈ 499004\n",
      "min_count=200 -> 保留词数 ≈ 288258\n",
      "min_count=300 -> 保留词数 ≈ 211961\n",
      "min_count=500 -> 保留词数 ≈ 145749\n",
      "min_count=1000 -> 保留词数 ≈ 91143\n",
      "min_count=2000 -> 保留词数 ≈ 58937\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r\"D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\all_cleaned_txts_str_processed.txt\"\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "# 先获取文件总行数（用来初始化 tqdm）\n",
    "def count_lines(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for i, _ in enumerate(f, 1):\n",
    "            pass\n",
    "    return i\n",
    "\n",
    "total_lines = count_lines(input_path)\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, total=total_lines, desc=\"统计词频中\"):\n",
    "        tokens = line.strip().split()\n",
    "        counter.update(tokens)\n",
    "\n",
    "# 统计各 min_count 下的词数\n",
    "for threshold in [5, 10, 50, 100, 200, 300, 500, 1000, 2000]:\n",
    "    retained = sum(1 for count in counter.values() if count >= threshold)\n",
    "    print(f\"min_count={threshold} -> 保留词数 ≈ {retained}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87d8834-32c3-4af8-8d29-1e1ccd57ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📚 Training Word2Vec: 100%|███████████████████████████████████████████████████████| 10/10 [23:07:05<00:00, 8322.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Word2Vec 模型训练完成，模型与词向量已保存。\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 自定义回调类：训练进度可视化\n",
    "class TQDMProgressBar(CallbackAny2Vec):\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.pbar = tqdm(total=epochs, desc=\"📚 Training Word2Vec\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def on_train_end(self, model):\n",
    "        self.pbar.close()\n",
    "\n",
    "\n",
    "# 设置语料路径\n",
    "input_path = r\"D:\\zhenfeng zhou\\自由的代价\\扩展种族主义词典\\full articles\\全文\\数据太大分开清洗\\all_cleaned_txts_str_processed.txt\"\n",
    "\n",
    "# 流式加载语料（每行为一个句子，空格分词）\n",
    "sentences = LineSentence(input_path)\n",
    "\n",
    "# 设置 epoch 次数\n",
    "epochs = 10\n",
    "\n",
    "# 训练模型\n",
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=300,\n",
    "    window=6,\n",
    "    negative=10,\n",
    "    min_count=500,\n",
    "    workers=32,\n",
    "    sg=1,\n",
    "    epochs=epochs,\n",
    "    compute_loss=True,\n",
    "    callbacks=[TQDMProgressBar(epochs)])\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"word2vec_large_300million.model\")\n",
    "model.wv.save_word2vec_format(\"word2vec_large_vectors_300million.txt\", binary=False)\n",
    "\n",
    "print(\"✅ Word2Vec 模型训练完成，模型与词向量已保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f77cf-e884-4a97-9ccc-182861080710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
