{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba99ffb0",
   "metadata": {},
   "source": [
    "## 基于超大型文本语料的seed word扩展计算——PMI路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa9bc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import json\n",
    "import traceback\n",
    "import threading\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0c735-58ad-47c8-ae7d-9e094c725687",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1800_1910 = pd.read_csv(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格文本及结果\\1800_1910_american_story.csv')   \n",
    "# df_1800_1910.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c02df-5616-44ef-9dc5-7ddc2b6d9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_1800_1910)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22951e6-75c3-490c-9233-224a9e1229df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 分成四份并写入本地  获取总行数和每份的大小\n",
    "# num_parts = 4\n",
    "# chunk_size = len(df_1800_1910) // num_parts\n",
    "\n",
    "# # 按照顺序划分并保存\n",
    "# for i in range(num_parts):\n",
    "#     start = i * chunk_size\n",
    "#     end = None if i == num_parts - 1 else (i + 1) * chunk_size\n",
    "#     df_part = df_1800_1910.iloc[start:end]\n",
    "    \n",
    "#     output_path = fr'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\1800_1910_newspapers_text_{i + 1}.csv'\n",
    "#     df_part.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb439e-1cdc-4da9-bc51-7df9a97565a4",
   "metadata": {},
   "source": [
    "### 随机选择1千万，大约20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73512da-102a-4947-816b-d1d6df1b40dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Page_number</th>\n",
       "      <th>Newspaper_name</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Author</th>\n",
       "      <th>Article_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18732542</th>\n",
       "      <td>2_1885-05-31_p1_sn85038614_00175030876_1885053...</td>\n",
       "      <td>1885-05-31</td>\n",
       "      <td>p1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>and tendency as to produce moral\\ncertainty Of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34358566</th>\n",
       "      <td>42_1897-10-13_p9_sn85042461_00280769757_189710...</td>\n",
       "      <td>1897-10-13</td>\n",
       "      <td>p9</td>\n",
       "      <td>The herald.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>= y y\\n\\n\\nThe annual report of the San Diego,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Article_id        Date  \\\n",
       "18732542  2_1885-05-31_p1_sn85038614_00175030876_1885053...  1885-05-31   \n",
       "34358566  42_1897-10-13_p9_sn85042461_00280769757_189710...  1897-10-13   \n",
       "\n",
       "         Page_number Newspaper_name Headline Author  \\\n",
       "18732542          p1            NaN      NaN    NaN   \n",
       "34358566          p9    The herald.      NaN    NaN   \n",
       "\n",
       "                                               Article_body  \n",
       "18732542  and tendency as to produce moral\\ncertainty Of...  \n",
       "34358566  = y y\\n\\n\\nThe annual report of the San Diego,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_1800_1910_sample = df_1800_1910.sample(n=10000000, random_state=42) \n",
    "# df_1800_1910_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1330ac53-701c-45e9-a35b-2353dd4ac62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 df_1800_1910_sample 保存为文件\n",
    "\n",
    "# df_1800_1910_sample.to_csv(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格文本及结果\\sample 10000\\1800_1910_sample_10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b830ae10-7b3f-4d70-8625-2eadc150a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow\n",
    "# df_1800_1910.to_parquet(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\df_1800_1910.parquet', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8cc3c6",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf3c3995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\supervisor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('state_union')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcc5e5-cde4-4402-8629-3f0bc4b08bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 对1800-1910年的语料初步筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9270180c-7a16-408b-8733-f006698a23da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from pmi_match_function import process_chunk, init_worker\n",
    "\n",
    "def parallel_filter_texts_pipe(texts, batch_size=100000, chunk_size=1000, max_workers=32):\n",
    "    \"\"\"并行批量处理文本，返回筛选后的结果\"\"\"\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "    filtered_all = []\n",
    "\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start:start + batch_size]\n",
    "        chunks = [batch[i:i + chunk_size] for i in range(0, len(batch), chunk_size)]\n",
    "\n",
    "        with mp.Pool(processes=max_workers, initializer=init_worker) as pool:\n",
    "            results = [\n",
    "                pool.apply_async(process_chunk, args=(chunk, progress_queue))\n",
    "                for chunk in chunks\n",
    "            ]\n",
    "\n",
    "            with tqdm(total=len(batch), desc=f\"Filtering [{start}-{start + len(batch)}]\") as pbar:\n",
    "                for _ in range(len(batch)):\n",
    "                    progress_queue.get()\n",
    "                    pbar.update(1)\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        for r in results:\n",
    "            filtered_all.extend([res for res in r.get() if res is not None])\n",
    "\n",
    "    return filtered_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a026f3-60b6-4d66-b2b1-46fad5aa5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # df_1800_1910 = pd.read_parquet(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\df_1800_1910.parquet\")\n",
    "    df_1800_1910 = pd.read_csv(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格文本及结果\\1800_1910_american_story.csv') \n",
    "    print(len(df_1800_1910))\n",
    "\n",
    "    print(1)\n",
    "\n",
    "    texts = df_1800_1910[\"Article_body\"].tolist()\n",
    "\n",
    "    print(2)\n",
    "    \n",
    "    filtered_texts = parallel_filter_texts_pipe(\n",
    "        texts,\n",
    "        batch_size=100000,\n",
    "        chunk_size=1000,\n",
    "        max_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f9b2cc-8443-4379-8774-cd763ba3f97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12851777"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88726f9d-a3f6-49ed-8edf-64585491a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1800_1910_2_select = df_1800_1910_2.loc[df_1800_1910_2['Article_body'].isin(filtered_texts)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1592e1a-4bc4-4427-aab6-ffc8e53b7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# 保存\n",
    "with open( r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\1800_1910_filtered_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de3bcf3a-8244-4d61-9f3a-b144e14881bd",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 构建最终 DataFrame\n",
    "# df_1800_1910_sample_select = df_1800_1910_sample.loc[df_1800_1910_sample['Article_body'].isin(filtered_texts)].copy()\n",
    "# #df_1800_1910['Article_body'] = filtered_texts\n",
    "\n",
    "# print(\"最终保留文章数：\", len(df_1800_1910_sample_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e05f7e-e032-48a8-bec0-b98ee9bf4316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 df_1800_1910_sample_select 保存为 Parquet 文件\n",
    "# df_1800_1910_sample_select.to_parquet(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\df_1800_1910_sample_select.parquet', compression='snappy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8930e8",
   "metadata": {},
   "source": [
    "#### 对提取的1800_1910的语料进行仔细清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73251eb5-812e-4b85-ac9a-e2f6a25379dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 Parquet 文件\n",
    "# df_1800_1910_select_2 = pd.read_csv(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\1800_1910_newspapers_text_2_select_true.csv')\n",
    "\n",
    "# df_1800_1910_select_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9baef66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_cleaner.py\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from pmi_match_function import process_text_2,init_worker_2\n",
    "\n",
    "def parallel_clean_texts(texts, max_workers):\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "\n",
    "    with mp.Pool(processes=max_workers,initializer=init_worker_2) as pool:\n",
    "        results = [\n",
    "            pool.apply_async(process_text_2, args=(text, progress_queue))\n",
    "            for text in texts]\n",
    "\n",
    "        cleaned_texts = []\n",
    "        with tqdm(total=len(texts), desc=\"Cleaning texts\") as pbar:\n",
    "            for _ in range(len(texts)):\n",
    "                progress_queue.get()\n",
    "                pbar.update(1)\n",
    "\n",
    "        for r in results:\n",
    "            cleaned_texts.append(r.get())\n",
    "\n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3fbb9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning texts: 100%|███████████████████████████████████████████████████| 12851777/12851777 [5:51:50<00:00, 608.78it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # texts = df_1800_1910_select_2['Article_body'].tolist()\n",
    "     import pickle\n",
    "     with open(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\1800_1910_filtered_texts.pkl', 'rb') as f:\n",
    "        texts = pickle.load(f)\n",
    "\n",
    "     print(1)\n",
    "\n",
    "     cleaned = parallel_clean_texts(texts, max_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d89087-9f04-436f-9737-25fa0fa41923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "import pickle\n",
    "with open( r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\1800_1910_cleaned_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(cleaned, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0d763d-a760-4805-afb9-b69a6242c5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12851777"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9915050-413b-41d8-813e-f3304818bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出本地，以保存避免重来\n",
    "# df_1800_1910_select_2.to_csv(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\df_1800_1910_select_cleaned_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d5932-dc59-40ed-82ad-9e8af9027994",
   "metadata": {},
   "source": [
    "#### 把清洗后的四个文件合并到一起,并单独提取出cleaned_Article_body和article_id组成新的两列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "568c9d97-78c6-4f2d-834f-f90748b20f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 读取第一个文件\n",
    "# df1 = pd.read_csv(r'D:\\OneDrive - 南京大学\\PMI计算\\df_1800_1910_select_cleaned_1.csv')\n",
    "# print(1)\n",
    "\n",
    "# # 读取第四个文件\n",
    "# df2 = pd.read_csv(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\df_1800_1910_select_cleaned_2.csv\")\n",
    "# print(2)\n",
    "\n",
    "# # 读取第二个文件\n",
    "# df3 = pd.read_csv(r'D:\\OneDrive - 南京大学\\PMI计算\\df_1800_1910_select_cleaned_3.csv')\n",
    "# print(3)\n",
    "\n",
    "# # 读取第三个文件\n",
    "# df4 = pd.read_csv(r'D:\\OneDrive - 南京大学\\PMI计算\\df_1800_1910_select_cleaned_4.csv')\n",
    "# print(4)\n",
    "\n",
    "# # 合并数据框\n",
    "# df_1800_1910_select_cleaned_all = pd.concat([df1, df2, df3,df4], axis=0)\n",
    "# print(5)\n",
    "\n",
    "# # 保存合并后的文件（可选）\n",
    "# df_1800_1910_select_cleaned_all.to_csv(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\df_1800_1910_select_cleaned_all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95ce2576-5dae-46f9-a3cb-5f28550b3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开数据\n",
    "# import pandas as pd\n",
    "\n",
    "# df_1800_1910_select_cleaned_all = pd.read_csv(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\df_1800_1910_select_cleaned_all.csv')\n",
    "# print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de91ac1f-dff9-498b-b93e-e94396499ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1800_1910_select_cleaned_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb9f8c08-4719-4a36-b5d1-0e51954471ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取指定的两列\n",
    "# new_df = df_1800_1910_select_cleaned_all[['Article_id', 'cleaned_Article_body']]\n",
    "\n",
    "# # 保存为新的CSV文件\n",
    "# new_df.to_csv(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\df_1800_1910_cleaned_Article_body.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f185ae9-a4ba-49c9-a6a2-4dabd33d08f3",
   "metadata": {},
   "source": [
    "#### 移除频率小于50的words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a03c285-50b7-4569-9b8e-222a0dc72cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from pmi_match_function import remove_rare_tokens_batch\n",
    "\n",
    "\n",
    "def build_token_counter(texts):\n",
    "    counter = Counter()\n",
    "    for text in tqdm(texts, desc=\"Counting tokens\"):\n",
    "        tokens = text.split() \n",
    "        counter.update(tokens)\n",
    "        del tokens\n",
    "    return counter\n",
    "\n",
    "\n",
    "def split_batches(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]\n",
    "\n",
    "def run_remove_rare_tokens_parallel(texts, token_counts, min_freq, max_workers=100, batch_size=100000):\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(remove_rare_tokens_batch, batch, token_counts, min_freq)\n",
    "            for batch in split_batches(texts, batch_size)\n",
    "        ]\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Removing rare tokens\"):\n",
    "            cleaned_chunk = future.result()\n",
    "            results.extend(cleaned_chunk)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7052b03d-40d9-41cc-84c9-76905a2e4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 加载文件\n",
    "    # df_1800_1910_cleaned_article_body = pd.read_csv(\n",
    "    # r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\df_1800_1910_cleaned_Article_body.csv',\n",
    "    # dtype={'cleaned_Article_body': 'string'})\n",
    "\n",
    "\n",
    "    # df_1800_1910_cleaned_article_body.dropna(subset=['cleaned_Article_body'], inplace=True)\n",
    "    \n",
    "    # print(1)\n",
    "    \n",
    "    # texts = df_1800_1910_cleaned_article_body['cleaned_Article_body'].tolist()\n",
    "    # print(2)\n",
    "\n",
    "    import pickle\n",
    "    with open(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 旧版\\data\\换行符正确转空格并处理连字符\\sample 10000\\1800_1910_cleaned_texts.pkl', 'rb') as f:\n",
    "        texts = pickle.load(f)\n",
    "         \n",
    "    texts = [text for text in texts if text.strip()]\n",
    "    print(1)\n",
    "\n",
    "    token_counts = build_token_counter(texts)\n",
    "    print(2)\n",
    "    \n",
    "    cleaned_texts = run_remove_rare_tokens_parallel(\n",
    "        texts,\n",
    "        token_counts,\n",
    "        min_freq=100,\n",
    "        max_workers=50,\n",
    "        batch_size=100000)\n",
    "    \n",
    "    print(\"3. 稀有词移除完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d11409ca-6c82-4bbf-a89a-f8ae195d1a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12851775"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30973131-37e9-45c8-a9e8-64ac703feb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# 保存\n",
    "with open( r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\1800_1910_cleaned_removeraretokens.pkl', 'wb') as f:\n",
    "    pickle.dump(cleaned_texts,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59bf28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出本地，以保存避免重来\n",
    "# df_1800_1910_sample_select['cleaned_Article_body'] = cleaned_texts\n",
    "# df_1800_1910_sample_select.to_parquet(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\共用新闻文本语料（1800-1920）american story\\df_1800_1910_sample_select_cleaned_2.parquet', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9adb24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1800_1910_sample_select"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67000f-5be8-450b-b137-e5f3391e4db3",
   "metadata": {},
   "source": [
    "### 计算在一个语料库中，两个词的pmi(另一个match模块里已加入）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a76c6",
   "metadata": {},
   "source": [
    "### 将最常见2-grams连接并替换原语料库中单独出现的两个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586113c0",
   "metadata": {},
   "source": [
    "#### 获取最常见的2-grams(1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6698eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from pmi_match_function import extract_bigrams_worker\n",
    "\n",
    "def chunked_iterable(iterable, size):\n",
    "    \"\"\"将可迭代对象分块\"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(islice(it, size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "def get_top_bigrams_parallel(texts, max_workers=32, chunk_size=1000):\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "\n",
    "    total_bigram_counts = Counter()\n",
    "    chunk_generator = chunked_iterable(texts, chunk_size)\n",
    "    num_chunks = (len(texts) + chunk_size - 1) // chunk_size\n",
    "\n",
    "    with mp.Pool(processes=max_workers) as pool:\n",
    "        with tqdm(total=num_chunks, desc=\"Computing + Collecting bigrams\") as pbar:\n",
    "            active_results = []\n",
    "\n",
    "            for i, chunk in enumerate(chunk_generator):\n",
    "                # 保持并发任务不超过 max_workers，以免过度占用内存\n",
    "                while len(active_results) >= max_workers:\n",
    "                    done_results = [r for r in active_results if r.ready()]\n",
    "                    for r in done_results:\n",
    "                        total_bigram_counts.update(r.get())\n",
    "                        active_results.remove(r)\n",
    "                        progress_queue.get()\n",
    "                        pbar.update(1)\n",
    "\n",
    "                result = pool.apply_async(extract_bigrams_worker, args=(chunk, progress_queue))\n",
    "                active_results.append(result)\n",
    "\n",
    "            # 处理剩余的任务\n",
    "            for r in active_results:\n",
    "                total_bigram_counts.update(r.get())\n",
    "                progress_queue.get()\n",
    "                pbar.update(1)\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    # 选择前 top k%\n",
    "    # top_k = int(len(total_bigram_counts) * keep_percent)\n",
    "    # top_bigrams = [bigram for bigram, _ in total_bigram_counts.most_common(top_k)]\n",
    "    # bigrams = [bigram for bigram, _ in total_bigram_counts.most_common()]\n",
    "    return total_bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b676f428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing + Collecting bigrams: 100%|██████████████████████████████████████████████| 2012/2012 [10:41<00:00,  3.14it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with open(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 旧版\\data\\换行符正确转空格并处理连字符\\sample 10000\\1800_1910_cleaned_removeraretokens.pkl', 'rb') as f:\n",
    "        texts = pickle.load(f)\n",
    "\n",
    "    top_1_percent_bigrams = get_top_bigrams_parallel(\n",
    "        texts,\n",
    "        max_workers=32,\n",
    "        chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b29ae504-3f4d-4627-8270-cd9882d9f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  top_1_percent_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7e4d78f-9a64-4233-a63e-dfb62f710183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可选：保存结果\n",
    "with open( r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\top_1_percent_bigrams.pkl', 'wb') as f:\n",
    "    pickle.dump(top_1_percent_bigrams, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7557ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2923230"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_1_percent_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd5aea-4853-4af4-a01a-76acccc40df5",
   "metadata": {},
   "source": [
    "#### 计算最常见(1%)的2元组的PMI分数，并依据PMI分数选取前20%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0825e8-0d80-49a2-8d2f-980938f5e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from pmi_match_function import common_2gram_pmi_worker_chunk\n",
    "\n",
    "# -------- 构建前缀哈希表结构（提升查询速度） --------\n",
    "def build_bigram_index(bigrams):\n",
    "    index = defaultdict(set)\n",
    "    for w1, w2 in bigrams:\n",
    "        index[w1].add(w2)\n",
    "    return index\n",
    "    \n",
    "# -------- 主函数：分批处理、进度追踪 --------\n",
    "def build_pmi_model_parallel_batched(texts, target_bigrams, max_workers=50, batch_size=100000, chunk_size=2000):\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "\n",
    "    bigram_index = build_bigram_index(target_bigrams)\n",
    "\n",
    "    total_token_article_counts = Counter()\n",
    "    total_token_pair_article_counts = Counter()\n",
    "\n",
    "    for batch_start in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[batch_start:batch_start + batch_size]\n",
    "        text_chunks = [batch_texts[i:i + chunk_size] for i in range(0, len(batch_texts), chunk_size)]\n",
    "\n",
    "        with mp.Pool(processes=max_workers) as pool:\n",
    "            results = [\n",
    "                pool.apply_async(common_2gram_pmi_worker_chunk, args=(chunk, bigram_index, progress_queue))\n",
    "                for chunk in text_chunks\n",
    "            ]\n",
    "\n",
    "            # ✅ 每个 chunk 对应一次更新，更丝滑\n",
    "            with tqdm(total=len(text_chunks), desc=f\"Batch {batch_start // batch_size + 1}\") as pbar:\n",
    "                for _ in range(len(text_chunks)):\n",
    "                    progress_queue.get()\n",
    "                    pbar.update(1)\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        for result in results:\n",
    "            token_counts, token_pair_counts = result.get()\n",
    "            total_token_article_counts.update(token_counts)\n",
    "            total_token_pair_article_counts.update(token_pair_counts)\n",
    "\n",
    "    return {\n",
    "        \"token_counts\": total_token_article_counts,\n",
    "        \"pair_counts\": total_token_pair_article_counts,\n",
    "        \"article_count\": len(texts)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e7f903-f15b-4946-bd12-85d7f1ebc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\1800_1910_cleaned_removeraretokens.pkl', 'rb') as f:\n",
    "    texts = pickle.load(f)\n",
    "print(1)\n",
    "\n",
    "with open(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\top_1_percent_bigrams.pkl', 'rb') as f:\n",
    "    target_bigrams = pickle.load(f)\n",
    "print(2)\n",
    "\n",
    "pmi_result = build_pmi_model_parallel_batched(texts, target_bigrams, max_workers=50, batch_size=100000, chunk_size=2000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74a5a57-0474-4a06-b54b-c34be7c93a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\common_bigrams_frequent.pkl', 'wb') as f:\n",
    "    # 使用pickle.dump()方法将数据保存到文件中\n",
    "    pickle.dump(pmi_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35efbc20-4ab1-49e2-a0ed-b5863e8c942a",
   "metadata": {},
   "source": [
    "### 计算常见二元组的pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86a6a092-75e6-47a2-9f54-5ccf26e8225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from pmi_match_function import init_worker_pmi, compute_pmi_batch_global\n",
    "\n",
    "def chunkify(lst, chunk_size):\n",
    "    \"\"\"将大列表分块\"\"\"\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]\n",
    "\n",
    "def compute_pmi_parallel_batched(word_pairs, token_counts, pair_counts, article_count, max_workers=40, batch_size=50000):\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "    \n",
    "    batches = list(chunkify(word_pairs, batch_size))\n",
    "\n",
    "    with mp.Pool(\n",
    "        processes=max_workers,\n",
    "        initializer=init_worker_pmi,\n",
    "        initargs=(token_counts, pair_counts, article_count)\n",
    "    ) as pool:\n",
    "        results = [\n",
    "            pool.apply_async(compute_pmi_batch_global, args=(batch, progress_queue))\n",
    "            for batch in batches\n",
    "        ]\n",
    "\n",
    "        with tqdm(total=len(batches), desc=\"Computing PMI in batches\") as pbar:\n",
    "            for _ in range(len(batches)):\n",
    "                progress_queue.get()\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Flatten batched results\n",
    "        all_results = []\n",
    "        for r in results:\n",
    "            all_results.extend(r.get())\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc9ebaa-63da-46d2-b458-4ef954747780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI in batches: 100%|████████████████████████████████████████████████████████| 59/59 [00:07<00:00,  7.72it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\top_1_percent_bigrams.pkl\", 'rb') as f:\n",
    "    top_1_percent_bigrams = pickle.load(f)\n",
    "\n",
    "print(1)\n",
    "\n",
    "with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\common_bigrams_frequent.pkl\", 'rb') as f:\n",
    "    pmi_result = pickle.load(f)\n",
    "\n",
    "print(2)\n",
    "\n",
    "pmi_results = compute_pmi_parallel_batched(\n",
    "    word_pairs=top_1_percent_bigrams,\n",
    "    token_counts=pmi_result[\"token_counts\"],\n",
    "    pair_counts=pmi_result[\"pair_counts\"],\n",
    "    article_count=pmi_result[\"article_count\"],\n",
    "    max_workers=40,\n",
    "    batch_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f598ffae-a352-44b6-955b-7c1da319ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\common_bigram_pmi_results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pmi_results, f)\n",
    "\n",
    "# pmi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b564dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\common_bigram_pmi_results.pkl\", \"rb\") as f:\n",
    "    pmi_results = pickle.load(f)\n",
    "\n",
    "# 筛选出前20%的pmi的2-gram\n",
    "sorted_data = sorted(pmi_results, key=lambda x: x[1], reverse=True)\n",
    "total_count =len(sorted_data)\n",
    "top_20_percent_cutoff = int(total_count * 0.2)\n",
    "top_20_percent_bigrams = [bigram for bigram, _ in sorted_data[:top_20_percent_cutoff]]\n",
    "\n",
    "top_20_percent_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad13e9f-4fa3-412f-be4f-fc78da34b8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584646"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_20_percent_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab35f0a1-1fc1-45ff-8344-bfda6de6d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\top_20_percent_bigrams_pmi.pkl\", \"wb\") as f:\n",
    "    pickle.dump(top_20_percent_bigrams, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4fa4f",
   "metadata": {},
   "source": [
    "#### 将筛选出的前20%的2-gram变成一个单词，并替换原文本中的两个词。（whites,blacks,white,negro 分开不分开都跑一次结果进行比较)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16a3dd28-9c49-4062-a002-acb476b27a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pmi_match_function import substitute_bigrams_worker\n",
    "\n",
    "\n",
    "def parallel_substitute_bigrams(texts, bigram_map, max_workers=32, batch_size=100000, chunk_size=10000):\n",
    "    all_results = []\n",
    "\n",
    "    for batch_start in range(0, len(texts), batch_size):\n",
    "        batch = texts[batch_start: batch_start + batch_size]\n",
    "        chunks = [batch[i:i + chunk_size] for i in range(0, len(batch), chunk_size)]\n",
    "\n",
    "        manager = mp.Manager()\n",
    "        queue = manager.Queue()\n",
    "\n",
    "        with mp.Pool(processes=max_workers) as pool:\n",
    "            results = [\n",
    "                pool.apply_async(substitute_bigrams_worker, args=(chunk, bigram_map, queue))\n",
    "                for chunk in chunks\n",
    "            ]\n",
    "\n",
    "            with tqdm(total=len(batch), desc=f\"Batch {batch_start // batch_size + 1}\") as pbar:\n",
    "                for _ in range(len(batch)):\n",
    "                    queue.get()\n",
    "                    pbar.update(1)\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        for r in results:\n",
    "            all_results.extend(r.get())\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d2b989-1c2a-46c5-8f94-58f99a60576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\1800_1910_cleaned_removeraretokens.pkl\", \"rb\") as t:\n",
    "        texts = pickle.load(t)\n",
    "    print(1)\n",
    "    \n",
    "    with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\top_20_percent_bigrams_pmi.pkl\", \"rb\") as f:\n",
    "        top_20_percent_bigrams = pickle.load(f)\n",
    "\n",
    "    # # 在top_20_percent_bigrams中去除带有whites,blacks,white,negro的元组。\n",
    "    # keywords = {'whites', 'blacks', 'white', 'negro'}\n",
    "\n",
    "    # top_20_percent_bigrams = [\n",
    "    #     bigram for bigram in top_20_percent_bigrams\n",
    "    #     if not any(word in keywords for word in bigram)]\n",
    "\n",
    "\n",
    "    # 构建 bigram 映射字典\n",
    "    bigram_map = {bigram: f\"{bigram[0]}{bigram[1]}\" for bigram in top_20_percent_bigrams}\n",
    "\n",
    "    print(1)\n",
    "\n",
    "    # 并行替换\n",
    "    substitute_texts = parallel_substitute_bigrams(\n",
    "        texts,\n",
    "        bigram_map,\n",
    "        max_workers=32,\n",
    "        batch_size=100000,\n",
    "        chunk_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dd0657f-51d2-48a6-983e-e59422e6f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\substitute_texts_noclear_2gram.pkl\", \"wb\") as f:\n",
    "\n",
    "    pickle.dump(substitute_texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc86781-83bc-4459-94f0-76bccc58355b",
   "metadata": {},
   "source": [
    "#### 把white people/ black people合并成连一块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "090cb747-6a71-4eb5-923e-5ae33fb5e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pmi_match_function import merge_white_black_people_worker_chunk\n",
    "\n",
    "\n",
    "def parallel_merge_white_supremacy(texts, max_workers=32, batch_size=100000, chunk_size=10000):\n",
    "    all_results = []\n",
    "\n",
    "    for batch_start in range(0, len(texts), batch_size):\n",
    "        batch = texts[batch_start: batch_start + batch_size]\n",
    "        chunks = [batch[i:i + chunk_size] for i in range(0, len(batch), chunk_size)]\n",
    "\n",
    "        manager = mp.Manager()\n",
    "        queue = manager.Queue()\n",
    "\n",
    "        with mp.Pool(processes=max_workers) as pool:\n",
    "            results = [\n",
    "                pool.apply_async(merge_white_black_people_worker_chunk, args=(chunk, queue))\n",
    "                for chunk in chunks\n",
    "            ]\n",
    "\n",
    "            with tqdm(total=len(batch), desc=f\"Merging batch {batch_start // batch_size + 1}\") as pbar:\n",
    "                for _ in range(len(batch)):\n",
    "                    queue.get()\n",
    "                    pbar.update(1)\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        for r in results:\n",
    "            all_results.extend(r.get())\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dec215a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pickle\n",
    "    \n",
    "    with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\substitute_texts_noclear_2gram.pkl\", \"rb\") as f:\n",
    "        texts = pickle.load(f)\n",
    "    \n",
    "    print(1)\n",
    "    \n",
    "    merged_texts = parallel_merge_white_supremacy(texts, max_workers=32, batch_size=100000, chunk_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26ba6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\full sample\\substitute_texts_whitepeopleblackpeople_nocleargram.pkl\", \"wb\") as f:\n",
    "    \n",
    "    pickle.dump(merged_texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b3abf-7413-486c-92c8-e21c1e250abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 种子词whitesupremacy与corpus中的词的PMI计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ddebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_appear_word(texts_list):\n",
    "#     appear_words = set()\n",
    "#     for text in tqdm(texts_list, desc=\"appear words\"):\n",
    "#         appear_words.update(text.split())\n",
    "\n",
    "#     # 移除种子词\n",
    "#     appear_words.discard(\"whitesupremacy\")\n",
    "\n",
    "#     return list(appear_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7035117f-5553-4e65-86f2-ecd0289b3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from collections import Counter\n",
    "# from itertools import combinations\n",
    "# from tqdm import tqdm\n",
    "# import multiprocessing as mp\n",
    "# from pmi_match_function import common_2gram_pmi_worker_chunk_2\n",
    "\n",
    "# def build_pmi_model_parallel_batched_2(texts, word1, max_workers=40, batch_size=100000, chunk_size=2000):\n",
    "#     manager = mp.Manager()\n",
    "#     progress_queue = manager.Queue()\n",
    "\n",
    "#     total_token_article_counts = Counter()\n",
    "#     total_token_pair_article_counts = Counter()\n",
    "\n",
    "#     for batch_start in range(0, len(texts), batch_size):\n",
    "#         batch_texts = texts[batch_start:batch_start + batch_size]\n",
    "#         # 分块：每个 worker 处理 chunk_size 篇文章\n",
    "#         text_chunks = [batch_texts[i:i + chunk_size] for i in range(0, len(batch_texts), chunk_size)]\n",
    "\n",
    "#         with mp.Pool(processes=max_workers) as pool:\n",
    "#             results = [\n",
    "#                 pool.apply_async(common_2gram_pmi_worker_chunk_2, args=(chunk, word1, progress_queue))\n",
    "#                 for chunk in text_chunks]\n",
    "\n",
    "#             with tqdm(total=len(batch_texts), desc=f\"Batch {batch_start // batch_size + 1}\") as pbar:\n",
    "#                 for _ in range(len(batch_texts)):\n",
    "#                     progress_queue.get()\n",
    "#                     pbar.update(1)\n",
    "\n",
    "#             pool.close()\n",
    "#             pool.join()\n",
    "\n",
    "#         for result in results:\n",
    "#             token_counts, token_pair_counts = result.get()\n",
    "#             total_token_article_counts.update(token_counts)\n",
    "#             total_token_pair_article_counts.update(token_pair_counts)\n",
    "\n",
    "#     return {\n",
    "#         \"token_counts\": total_token_article_counts,\n",
    "#         \"pair_counts\": total_token_pair_article_counts,\n",
    "#         \"article_count\": len(texts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a5a4be-f6f8-446d-8ee7-6321d7056e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pmi_match_function import compute_pmi_batch_worker_global, init_globals\n",
    "\n",
    "# def parallel_pmi_for_seed(word1, appear_words, token_counts, pair_counts, article_count, max_workers=32, batch_size=100000):\n",
    "#     token_probs = {w: c / article_count for w, c in token_counts.items()}\n",
    "#     pair_probs = {pair: c / article_count for pair, c in pair_counts.items()}\n",
    "\n",
    "#     manager = mp.Manager()\n",
    "#     progress_queue = manager.Queue()\n",
    "\n",
    "#     word_batches = [appear_words[i:i+batch_size] for i in range(0, len(appear_words), batch_size)]\n",
    "\n",
    "#     with mp.Pool(processes=max_workers, initializer=init_globals, initargs=(token_probs, pair_probs)) as pool:\n",
    "#         results = [\n",
    "#             pool.apply_async(\n",
    "#                 compute_pmi_batch_worker_global,\n",
    "#                 args=(batch, word1, progress_queue))\n",
    "#             for batch in word_batches]\n",
    "\n",
    "#         with tqdm(total=len(appear_words), desc=f\"Computing PMI for {word1}\") as pbar:\n",
    "#             for _ in range(len(appear_words)):\n",
    "#                 progress_queue.get()\n",
    "#                 pbar.update(1)\n",
    "\n",
    "#         all_results = []\n",
    "#         for res in results:\n",
    "#             all_results.extend(res.get())\n",
    "\n",
    "#     return {word1: all_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aef8a9c-5798-4b81-98d0-4006102a509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # 打开数据\n",
    "#     with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格文本及结果\\sample 10000\\substitute_texts_whitesupremacy.pkl\", \"rb\") as f:\n",
    "#         texts = pickle.load(f)\n",
    "    \n",
    "#     # 运行第一步：构建 PMI 统计数据\n",
    "#     pmi_data =  build_pmi_model_parallel_batched_2(texts, \"whitesupremacy\", max_workers=40, batch_size=100000, chunk_size=2000)\n",
    "\n",
    "#     # 运行第二步：提取所有单词\n",
    "#     appear_words = calculate_appear_word(texts)\n",
    "\n",
    "#     print(1)\n",
    "\n",
    "#     # 运行第三步：计算 PMI\n",
    "#     word1 = \"whitesupremacy\"\n",
    "#     dict_pmi = parallel_pmi_for_seed(\n",
    "#         word1=word1,\n",
    "#         appear_words=appear_words,\n",
    "#         token_counts=pmi_data[\"token_counts\"],\n",
    "#         pair_counts=pmi_data[\"pair_counts\"],\n",
    "#         article_count=pmi_data[\"article_count\"],\n",
    "#         max_workers=32,batch_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8bdeae-ea5b-461e-aeb0-9dcf1ee90064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存入本地\n",
    "# import pickle\n",
    "\n",
    "# with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格文本及结果\\sample 10000\\pmi_final_result.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(dict_pmi, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24b7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmi_sort = sorted(dict_pmi[\"whitesupremacy\"], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# pmi_sort[:100]\n",
    "\n",
    "# top_300_pmi = [filter_zero for filter_zero in top_300_pmi if filter_zero[1] > 0]\n",
    "# top_dict_threat[f'{item}'] = top_300_pmi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41935542-34e7-459e-a2fa-f1f51438aea1",
   "metadata": {},
   "source": [
    "### 计算与negro/blacks/black people相关的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789a38b9-8665-459b-929d-5f1654941f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开处理后的数据\n",
    "import pickle\n",
    "with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 旧版\\data\\换行符正确转空格并处理连字符\\sample 10000\\substitute_texts_whitepeopleblackpeople_nocleargram.pkl\", \"rb\") as f:\n",
    "    \n",
    "    texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc6ae85-636a-463d-b10b-a4db1d413601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_appear_word(texts_list):\n",
    "    appear_words = set()\n",
    "    for text in tqdm(texts_list, desc=\"appear words\"):\n",
    "        appear_words.update(text.split())\n",
    "\n",
    "    # 移除种子词\n",
    "    appear_words.discard(\"criminal\")\n",
    "\n",
    "    return list(appear_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d11fd66-0214-42fd-b8b2-7961e9a28ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from pmi_match_function import common_2gram_pmi_worker_chunk_2\n",
    "\n",
    "def build_pmi_model_parallel_batched_2(texts, word1, max_workers=40, batch_size=100000, chunk_size=2000):\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "\n",
    "    total_token_article_counts = Counter()\n",
    "    total_token_pair_article_counts = Counter()\n",
    "\n",
    "    for batch_start in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[batch_start:batch_start + batch_size]\n",
    "        # 分块：每个 worker 处理 chunk_size 篇文章\n",
    "        text_chunks = [batch_texts[i:i + chunk_size] for i in range(0, len(batch_texts), chunk_size)]\n",
    "\n",
    "        with mp.Pool(processes=max_workers) as pool:\n",
    "            results = [\n",
    "                pool.apply_async(common_2gram_pmi_worker_chunk_2, args=(chunk, word1, progress_queue))\n",
    "                for chunk in text_chunks]\n",
    "\n",
    "            with tqdm(total=len(batch_texts), desc=f\"Batch {batch_start // batch_size + 1}\") as pbar:\n",
    "                for _ in range(len(batch_texts)):\n",
    "                    progress_queue.get()\n",
    "                    pbar.update(1)\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        for result in results:\n",
    "            token_counts, token_pair_counts = result.get()\n",
    "            total_token_article_counts.update(token_counts)\n",
    "            total_token_pair_article_counts.update(token_pair_counts)\n",
    "\n",
    "    return {\n",
    "        \"token_counts\": total_token_article_counts,\n",
    "        \"pair_counts\": total_token_pair_article_counts,\n",
    "        \"article_count\": len(texts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6196b1ed-376e-4702-ad8b-833c1e2ebd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmi_match_function import compute_pmi_batch_worker_global, init_globals\n",
    "\n",
    "def parallel_pmi_for_seed(word1, appear_words, token_counts, pair_counts, article_count, max_workers=32, batch_size=100000):\n",
    "    token_probs = {w: c / article_count for w, c in token_counts.items()}\n",
    "    pair_probs = {pair: c / article_count for pair, c in pair_counts.items()}\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "\n",
    "    word_batches = [appear_words[i:i+batch_size] for i in range(0, len(appear_words), batch_size)]\n",
    "\n",
    "    with mp.Pool(processes=max_workers, initializer=init_globals, initargs=(token_probs, pair_probs)) as pool:\n",
    "        results = [\n",
    "            pool.apply_async(\n",
    "                compute_pmi_batch_worker_global,\n",
    "                args=(batch, word1, progress_queue))\n",
    "            for batch in word_batches]\n",
    "\n",
    "        with tqdm(total=len(appear_words), desc=f\"Computing PMI for {word1}\") as pbar:\n",
    "            for _ in range(len(appear_words)):\n",
    "                progress_queue.get()\n",
    "                pbar.update(1)\n",
    "\n",
    "        all_results = []\n",
    "        for res in results:\n",
    "            all_results.extend(res.get())\n",
    "\n",
    "    return {word1: all_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a18c7f-d4b0-4cf1-aa3b-5fa754d63295",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 打开数据\n",
    "    # with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格文本及结果\\sample 10000\\substitute_texts_whitesupremacy.pkl\", \"rb\") as f:\n",
    "    #     texts = pickle.load(f)\n",
    "    \n",
    "    # 运行第一步：构建 PMI 统计数据\n",
    "    pmi_data =  build_pmi_model_parallel_batched_2(texts, \"criminal\", max_workers=40, batch_size=100000, chunk_size=2000)\n",
    "\n",
    "    # 运行第二步：提取所有单词\n",
    "    appear_words = calculate_appear_word(texts)\n",
    "\n",
    "    print(1)\n",
    "\n",
    "    # 运行第三步：计算 PMI\n",
    "    word1 = \"criminal\"\n",
    "    dict_pmi = parallel_pmi_for_seed(\n",
    "        word1=word1,\n",
    "        appear_words=appear_words,\n",
    "        token_counts=pmi_data[\"token_counts\"],\n",
    "        pair_counts=pmi_data[\"pair_counts\"],\n",
    "        article_count=pmi_data[\"article_count\"],\n",
    "        max_workers=32,batch_size=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fa80ce7-b419-434b-8725-554aa73fc5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存入本地\n",
    "import pickle\n",
    "\n",
    "with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\sample 10000\\negro语义相关词\\pmi_final_result_negro_noclear2gram.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict_pmi, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64092cdb-960e-428b-83fe-84289b34fc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('equityjurisprudence', 4.1461837866709965),\n",
       " ('nolken', 4.1229269245067295),\n",
       " ('equitypleading', 4.093073961357048),\n",
       " ('liilecturer', 4.083706211353448),\n",
       " ('mootcourt', 3.955262525512748),\n",
       " ('pleadingpractice', 3.925395949049246),\n",
       " ('habitualcriminal', 3.9198022400830115),\n",
       " ('criminalcondemn', 3.8650978152046296),\n",
       " ('criminology', 3.8642323889282006),\n",
       " ('criminologist', 3.8572237587737237),\n",
       " ('courthon', 3.766251980567997),\n",
       " ('judgepickett', 3.764293116082664),\n",
       " ('criminalpunish', 3.729884336397122),\n",
       " ('administrationcriminal', 3.663394595128289),\n",
       " ('jurisdictioncivil', 3.6289084190571197),\n",
       " ('preventioncrime', 3.6175213682100775),\n",
       " ('justicefursman', 3.6121013007407385),\n",
       " ('crimimail', 3.560808006353188),\n",
       " ('crimecriminal', 3.5111253834007363),\n",
       " ('defendantcriminal', 3.5050031651843714)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi_sort = sorted(dict_pmi[\"criminal\"], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pmi_sort[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a2a8e-ab0c-4f63-943e-92860ece6b97",
   "metadata": {},
   "source": [
    "### 计算与whites/white/white people语义相关词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b41fe9-0658-4bd1-8cd9-f5e2745ebdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开处理后的数据\n",
    "import pickle\n",
    "\n",
    "with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 旧版\\data\\换行符正确转空格并处理连字符\\sample 10000\\1800_1910_cleaned_removeraretokens.pkl\", \"rb\") as f:\n",
    "   \n",
    "    \n",
    "    texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74cd7a39-e33f-4216-81da-9215dbeee2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011942"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d63c2bf-9a6a-4334-9b4c-402872132a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_appear_word(texts_list):\n",
    "    appear_words = set()\n",
    "    for text in tqdm(texts_list, desc=\"appear words\"):\n",
    "        appear_words.update(text.split())\n",
    "\n",
    "    # 移除种子词\n",
    "    appear_words.discard(\"criminal\")\n",
    "\n",
    "    return list(appear_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed72fad2-2090-402b-bd88-714e39d9946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from pmi_match_function import common_2gram_pmi_worker_chunk_2\n",
    "\n",
    "def build_pmi_model_parallel_batched_2(texts, word1, max_workers=40, batch_size=100000, chunk_size=2000):\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "\n",
    "    total_token_article_counts = Counter()\n",
    "    total_token_pair_article_counts = Counter()\n",
    "\n",
    "    for batch_start in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[batch_start:batch_start + batch_size]\n",
    "        # 分块：每个 worker 处理 chunk_size 篇文章\n",
    "        text_chunks = [batch_texts[i:i + chunk_size] for i in range(0, len(batch_texts), chunk_size)]\n",
    "\n",
    "        with mp.Pool(processes=max_workers) as pool:\n",
    "            results = [\n",
    "                pool.apply_async(common_2gram_pmi_worker_chunk_2, args=(chunk, word1, progress_queue))\n",
    "                for chunk in text_chunks]\n",
    "\n",
    "            with tqdm(total=len(batch_texts), desc=f\"Batch {batch_start // batch_size + 1}\") as pbar:\n",
    "                for _ in range(len(batch_texts)):\n",
    "                    progress_queue.get()\n",
    "                    pbar.update(1)\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        for result in results:\n",
    "            token_counts, token_pair_counts = result.get()\n",
    "            total_token_article_counts.update(token_counts)\n",
    "            total_token_pair_article_counts.update(token_pair_counts)\n",
    "\n",
    "    return {\n",
    "        \"token_counts\": total_token_article_counts,\n",
    "        \"pair_counts\": total_token_pair_article_counts,\n",
    "        \"article_count\": len(texts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "570fce14-2df3-421d-b9ce-902250e19167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmi_match_function import compute_pmi_batch_worker_global, init_globals\n",
    "\n",
    "def parallel_pmi_for_seed(word1, appear_words, token_counts, pair_counts, article_count, max_workers=32, batch_size=100000):\n",
    "    token_probs = {w: c / article_count for w, c in token_counts.items()}\n",
    "    pair_probs = {pair: c / article_count for pair, c in pair_counts.items()}\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "\n",
    "    word_batches = [appear_words[i:i+batch_size] for i in range(0, len(appear_words), batch_size)]\n",
    "\n",
    "    with mp.Pool(processes=max_workers, initializer=init_globals, initargs=(token_probs, pair_probs)) as pool:\n",
    "        results = [\n",
    "            pool.apply_async(\n",
    "                compute_pmi_batch_worker_global,\n",
    "                args=(batch, word1, progress_queue))\n",
    "            for batch in word_batches]\n",
    "\n",
    "        with tqdm(total=len(appear_words), desc=f\"Computing PMI for {word1}\") as pbar:\n",
    "            for _ in range(len(appear_words)):\n",
    "                progress_queue.get()\n",
    "                pbar.update(1)\n",
    "\n",
    "        all_results = []\n",
    "        for res in results:\n",
    "            all_results.extend(res.get())\n",
    "\n",
    "    return {word1: all_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f04935-8674-469c-bbb2-745df27cbc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 打开数据\n",
    "    # with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格文本及结果\\sample 10000\\substitute_texts_whitesupremacy.pkl\", \"rb\") as f:\n",
    "    #     texts = pickle.load(f)\n",
    "    \n",
    "    # 运行第一步：构建 PMI 统计数据\n",
    "    pmi_data =  build_pmi_model_parallel_batched_2(texts, \"criminal\", max_workers=40, batch_size=100000, chunk_size=2000)\n",
    "\n",
    "    # 运行第二步：提取所有单词\n",
    "    appear_words = calculate_appear_word(texts)\n",
    "\n",
    "    print(1)\n",
    "\n",
    "    # 运行第三步：计算 PMI\n",
    "    word1 = \"criminal\"\n",
    "    dict_pmi = parallel_pmi_for_seed(\n",
    "        word1=word1,\n",
    "        appear_words=appear_words,\n",
    "        token_counts=pmi_data[\"token_counts\"],\n",
    "        pair_counts=pmi_data[\"pair_counts\"],\n",
    "        article_count=pmi_data[\"article_count\"],\n",
    "        max_workers=32,batch_size=100000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eafd2f2a-1f1f-4223-a6b2-ba501ef969ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135670"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(appear_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e717e89-fe5e-4e48-b973-052f36f8ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存入本地\n",
    "import pickle\n",
    "\n",
    "with open(r\"D:\\zhenfeng zhou\\自由的代价\\扩展 white supremacy 词袋\\data\\换行符正确转空格并处理连字符\\sample 10000\\white people 语义相关词\\pmi_final_result_whitepeople_noclear2gram.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict_pmi, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7889d8-b211-41cc-9aed-eae7cb6f6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmi_sort = sorted(dict_pmi[\"criminal\"], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# pmi_sort[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea6c0a-68be-402b-bdc0-d793b9e206c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
