{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c91370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging \n",
    "import random\n",
    "import urllib3\n",
    "import requests\n",
    "import threading\n",
    "import traceback\n",
    "from tqdm.notebook import tqdm\n",
    "from pyquery import PyQuery as pq\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e483210",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe62273",
   "metadata": {},
   "outputs": [],
   "source": [
    " headers= {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"cache-control\": \"no-cache\",\n",
    "    \"pragma\": \"no-cache\",\n",
    "        # 加\"\\\"转义，确保值正确。\n",
    "    \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"117\\\", \\\"Not;A=Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"117\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"none\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"upgrade-insecure-requests\": \"1\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2942a4d4",
   "metadata": {},
   "source": [
    "### 获取元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42d588b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983_1.json已经存在\n",
      "1983_2.json已经存在\n",
      "1983_3.json已经存在\n",
      "1983_4.json已经存在\n",
      "1983_5.json已经存在\n",
      "1983_6.json已经存在\n",
      "1983_7.json已经存在\n",
      "1983_8.json已经存在\n",
      "1983_9.json已经存在\n",
      "1983_10.json已经存在\n",
      "1983_11.json已经存在\n",
      "1983_12.json已经存在\n"
     ]
    }
   ],
   "source": [
    "def obtain_metadata(start_year,end_year,headers):\n",
    "    for year in range(start_year,end_year+1): \n",
    "            for month in range(1,13):\n",
    "                api_key = \"你的api-key\"\n",
    "                output_directory = r\"/root/nyt/nyt_metadata\" \n",
    "                os.makedirs(output_directory, exist_ok=True)\n",
    "                file_name = f\"{year}_{month}.json\"\n",
    "                file_directory = f\"//root//nyt//nyt_metadata//{year}\"\n",
    "                os.makedirs(file_directory, exist_ok=True)\n",
    "                file_path = os.path.join(file_directory,file_name)\n",
    "                \n",
    "                if os.path.exists(file_path):\n",
    "                    print(file_name + \"已经存在\")\n",
    "                else:\n",
    "                    for _ in range(3):\n",
    "                        try:\n",
    "                            ip_pool = [{\"http\":\"http://lu6510291:X4TKZW@45.205.70.85:7777\",\"https\":\"http://lu6510291:X4TKZW@45.205.70.85:7777\"},\n",
    "                                       {\"http\":\"http://lu6510291:X4TKZW@154.61.160.247:7777\",\"https\":\"http://lu6510291:X4TKZW@154.61.160.247:7777\"}]\n",
    "                            url = \"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={api_key}\".format(year=year,month=month,api_key=api_key)\n",
    "                            response = requests.get(url,headers = headers,proxies = random.choice(ip_pool),timeout=20,verify=False,allow_redirects=True)\n",
    "                            if response.status_code == 200:                \n",
    "                                metadata = response.json()\n",
    "                                if \"fault\" in metadata:\n",
    "                                    print(f\"{file_name},请求到错误内容\")\n",
    "                                    sleep(3)\n",
    "                                else:\n",
    "                                    with open(file_path, 'w') as file:\n",
    "                                        json.dump(metadata, file, separators=(', ', ': '), indent=None)\n",
    "                                    time.sleep(3)\n",
    "                                break\n",
    "                            else:\n",
    "                                continue\n",
    "                        except Exception as e:\n",
    "                            traceback.print_exc() \n",
    "                            continue\n",
    "                    else:\n",
    "                        print(f\"AIP获取失败，{year}/{month}\")                        \n",
    "                        \n",
    "if __name__ == \"__main__\":\n",
    "    obtain_metadata(1983,1983,headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d6a26",
   "metadata": {},
   "source": [
    "### 提取元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ee78665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "def store_useful_metadata(start_year,end_year):\n",
    "    directory_first = r\"/root/nyt/store_useful_matedata\" \n",
    "    os.makedirs(directory_first, exist_ok=True)\n",
    "    try: \n",
    "        for year in range(start_year,end_year +1):\n",
    "            # 计数\n",
    "            a = 0\n",
    "            useful_metadata = []\n",
    "            folder_path = f\"/root/nyt/nyt_metadata/{year}\"\n",
    "            store_file = f\"{year}.json\"\n",
    "            store_filepath = os.path.join(directory_first,store_file )\n",
    "            \n",
    "            for filename in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path,'r',encoding = 'utf-8') as file:\n",
    "                    data = json.load(file)\n",
    "                    for item in data['response']['docs']:\n",
    "                        if item['web_url'] and \"https\" in item[\"web_url\"]:\n",
    "                           # 去除视频链接，交互页面链接、图片等。\n",
    "                           if not any(substring in item['web_url'] for substring in [\"/video/\", \"/interactive/\", \"/slideshow/\",\"/newsgraphics/\"]):\n",
    "                                metadata ={\n",
    "                                  \"url\" : item[\"web_url\"],\n",
    "                                  \"pub_date\" : item[\"pub_date\"],\n",
    "                                  \"headline\" : item[\"headline\"][\"main\"],\n",
    "                                  \"abstract\" : item[\"abstract\"],\n",
    "                                  \"author\"   : item[\"byline\"][\"original\"],\n",
    "                                 \"keywords\"  : ','.join([ word[\"value\"] for word in item[\"keywords\"]])\n",
    "                                  }\n",
    "                                useful_metadata.append(metadata)\n",
    "                    a += 1\n",
    "                    \n",
    "            with open(store_filepath,\"w\",encoding = 'utf-8') as file:\n",
    "                json.dump(useful_metadata, file, ensure_ascii=False, indent=2)    \n",
    "                print(a) \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    store_useful_metadata(1983,1983)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d907bb0",
   "metadata": {},
   "source": [
    "### 根据元数据爬取nytimes原文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792b7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYTimesCrawler:\n",
    "    # 初始化实例\n",
    "    def __init__(self,year):\n",
    "        self.year = year\n",
    "        self.cookies = self.open_cookies()\n",
    "        self.proxy,self.request_count =self.get_proxy()\n",
    "        \n",
    "    # 构造随机化请求头  \n",
    "    def get_header(self):\n",
    "        with open(f\"/root/nyt_crawl/user_agent/user_agent_{self.year}.json\",\"r\") as file:\n",
    "            content = file.read()\n",
    "        content_with_double_quotes = content.replace(\"'\", \"\\\"\")\n",
    "        user_agent = json.loads(content_with_double_quotes)\n",
    "        \n",
    "        headers= {\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"accept-encoding\": \"gzip, deflate, br\",\n",
    "        \"accept-language\": \"en-US,en;q=0.9\",\n",
    "        \"cache-control\": \"no-cache\",\n",
    "        \"pragma\": \"no-cache\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-fetch-dest\": \"document\",\n",
    "        \"sec-fetch-mode\": \"navigate\",\n",
    "        \"sec-fetch-site\": \"none\",\n",
    "        \"sec-fetch-user\": \"?1\",\n",
    "        \"upgrade-insecure-requests\": \"1\",\n",
    "        \"user-agent\": random.choice(user_agent)}\n",
    "        return headers\n",
    "\n",
    "    # 打开储存的提取过后的metadata，以年为单位。\n",
    "    def open_metadatafile(self):\n",
    "        file_path = f\"/root/nyt/store_useful_matedata/{self.year}.json\"\n",
    "        sort_data = None\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                sort_data = sorted(data, key=lambda x: datetime.strptime(x[\"pub_date\"], \"%Y-%m-%dT%H:%M:%S%z\"))\n",
    "        else:\n",
    "            print(f\"{file_path}, 不存在此路径\")\n",
    "        return sort_data\n",
    "    \n",
    "    #自动切换cookies\n",
    "    def open_cookies(self):\n",
    "        update_cookies_path = f\"/root/nyt_crawl/cookies/cookies_{self.year}/update_cookies.json\"\n",
    "        if os.path.exists(update_cookies_path):\n",
    "            with open(update_cookies_path,\"r\") as file:\n",
    "                cookies_path = json.load(file)\n",
    "            cookies_path.append(cookies_path.pop(0))\n",
    "            \n",
    "            with open(cookies_path[0],'r') as file:\n",
    "                cookies_content = json.load(file)\n",
    "                \n",
    "            with open(update_cookies_path,\"w\") as file:\n",
    "                json.dump(cookies_path,file)\n",
    "        else:\n",
    "            folder_path = f\"/root/nyt_crawl/cookies/cookies_{self.year}\"\n",
    "            files = []\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                if os.path.exists(file_path):\n",
    "                    files.append(file_path)\n",
    "            with open(files[0],'r') as file:\n",
    "                cookies_content = json.load(file)\n",
    "                \n",
    "            with open(update_cookies_path,\"w\") as file:\n",
    "                json.dump(files,file) \n",
    "        return cookies_content\n",
    "    \n",
    "    \n",
    "    # 自动切换ip\n",
    "    def get_proxy(self):\n",
    "        ip_pool_path = f\"/root/nyt_crawl/ip_pool_{self.year}.json\"\n",
    "        if os.path.exists(ip_pool_path):\n",
    "            with open(ip_pool_path,\"r\") as file:\n",
    "                ip_pool = json.load(file)\n",
    "            ip_pool.append(ip_pool.pop(0))\n",
    "            \n",
    "            proxy = ip_pool[0]\n",
    "            request_count = 0\n",
    "            \n",
    "            with open(ip_pool_path,\"w\") as file:\n",
    "                json.dump(ip_pool,file)\n",
    "        else:           \n",
    "            ip_pool = [{\"http\":\"http://lu6510291:X4TKZW@45.205.70.85:7777\",\"https\":\"http://lu6510291:X4TKZW@45.205.70.85:7777\"},\n",
    "                       {\"http\":\"http://lu6510291:X4TKZW@154.61.160.247:7777\",\"https\":\"http://lu6510291:X4TKZW@154.61.160.247:7777\"}]\n",
    "            with open(ip_pool_path,'w') as file:\n",
    "                      json.dump(ip_pool,file)\n",
    "            proxy = ip_pool[0]\n",
    "            request_count = 0\n",
    "                  \n",
    "        return proxy,request_count\n",
    "    \n",
    "    # 创建文件夹及文件\n",
    "    def initialize_files_path(self):\n",
    "        directory_first =  r'/root/nyt/nyt_data'\n",
    "        os.makedirs(directory_first, exist_ok=True)\n",
    "        directory_second = f'/root/nyt/nyt_data/{self.year}'\n",
    "        os.makedirs(directory_second, exist_ok=True)\n",
    "        \n",
    "        errorurl_path = os.path.join(directory_second,\"errorurl.txt\")\n",
    "        verify_access_path = os.path.join(directory_second,\"verify_access.txt\")\n",
    "        directory_full = os.path.join(directory_second,\"full_article\")\n",
    "        os.makedirs(directory_full, exist_ok=True)\n",
    "        no_pub_dateandcontent_path = os.path.join(directory_second,\"no_pub_dateandcontent.txt\")\n",
    "    \n",
    "        return errorurl_path,verify_access_path,directory_full, no_pub_dateandcontent_path\n",
    "    \n",
    "    # 爬取一篇文章的内容\n",
    "    def crawl_onearticle_content(self,article_url,errorurl_path,cookie,proxy):\n",
    "        resp = None \n",
    "        for _ in range(4):\n",
    "            try:\n",
    "                session = requests.session()\n",
    "                resp = session.get(article_url,headers=self.get_header(),proxies=proxy,cookies=cookie,timeout=(30,60), verify=False)\n",
    "                \n",
    "                if resp.status_code != 200:\n",
    "                    if resp.status_code == 403:\n",
    "                        self.cookies = self.open_cookies()\n",
    "                        cookie = self.cookies\n",
    "                        self.proxy,self.request_count =self.get_proxy()\n",
    "                        proxy,request_count =  self.proxy,self.request_count\n",
    "                        continue\n",
    "                    else: \n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                else:\n",
    "                    resp.encoding = 'utf-8'\n",
    "                    get_content = resp.text\n",
    "                    if get_content:\n",
    "                        break\n",
    "                    else:\n",
    "                        time.sleep(2)\n",
    "                        continue\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                traceback.print_exc()\n",
    "                print(e,proxy)\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "        else:\n",
    "            if resp is None:\n",
    "                pass\n",
    "            else:\n",
    "                if resp.status_code == 403:\n",
    "                    time.sleep(1800)\n",
    "                \n",
    "            print(f\"{article_url},获取内容失败\")\n",
    "            with open(errorurl_path,\"a\",encoding = \"utf-8\") as file:\n",
    "                file.write(article_url + \"\\n\")\n",
    "            return None,None,None\n",
    "\n",
    "        query = pq(get_content)\n",
    "        \n",
    "        if \"www.nytimes.com\" in article_url:\n",
    "            articleid_elem = query.find('[name=\"articleid\"]')  \n",
    "            content_elem = query.find(\"section[name='articleBody']\").children('div.StoryBodyCompanionColumn')\n",
    "            content_elem.find('p.etfikam0,div[data-testid=\"photoviewer-wrapper\"]').remove()\n",
    "            citation_elem = content_elem.find('p a')\n",
    "            \n",
    "        else:\n",
    "            articleid_elem = query.find('[name=\"blogpostid\"]')\n",
    "            content_elem = query(\"div[id='content']\").find('div.entry-content')\n",
    "            content_elem.find('div[class*=w],div.entry-section').remove()\n",
    "            citation_elem = content_elem.find(\"p a\")\n",
    "        \n",
    "        articleid = str(articleid_elem.attr(\"content\")) if articleid_elem else None\n",
    "        content = str(content_elem.text()) if content_elem else None\n",
    "        citation = \",\".join([f\"{pq(a).text()}||{pq(a).attr('href')}\" for a in citation_elem]) if citation_elem else None\n",
    "        return articleid,content,citation\n",
    "    \n",
    "         \n",
    "    # 检测是否出现爬取到付费墙的情况  \n",
    "    def check_content(self,article_url,errorurl_path,cookie,proxy,articleid,content,citation,verify_access_path):\n",
    "        try:\n",
    "            if \"Thank you for your patience while we verify access.\" in content:\n",
    "                for _ in range(2): \n",
    "                    articleid,content,citation = self.crawl_onearticle_content(article_url,errorurl_path,cookie,proxy)\n",
    "                    if content is None:\n",
    "                        break\n",
    "                    elif content and \"Thank you for your patience while we verify access.\" not in content:\n",
    "                        break\n",
    "                    else:\n",
    "                        time.sleep(2)\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"出现获取到验证登录页面的情况，{article_url}\")\n",
    "                    with open(verify_access_path, \"a\", encoding=\"utf-8\") as file:\n",
    "                        file.write(article_url + \"\\n\")\n",
    "                    return \"\", \"\",\"\"\n",
    "            return articleid,content,citation\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    \n",
    "    # 规模爬取\n",
    "    def crawl_nytime_main(self,sort_data):\n",
    "        # 创建文件夹\n",
    "        errorurl_path,verify_access_path,directory_full,no_pub_dateandcontent_path = self.initialize_files_path()\n",
    "        pbar = tqdm(sort_data,desc = f\"Processing:{self.year}\",initial=0)\n",
    "        \n",
    "        # 配置cookies和proxy.\n",
    "        cookie = self.cookies\n",
    "        proxy,request_count = self.proxy,self.request_count\n",
    "       \n",
    "        \n",
    "        for index,item in enumerate(pbar):\n",
    "            article_url = str(item[\"url\"])\n",
    "            pub_date    = str(item[\"pub_date\"])\n",
    "            headline    = str(item[\"headline\"])\n",
    "            abstract    = str(item[\"abstract\"])\n",
    "            author      = str(item[\"author\"])\n",
    "            keywords    = str(item[\"keywords\"])\n",
    "            pbar.set_postfix(index=f\"{index},{pub_date}\")\n",
    "            \n",
    "            # 给每个ip分配任务，延缓封锁。\n",
    "            if request_count >40:\n",
    "                self.proxy,self.request_count =self.get_proxy()\n",
    "                proxy,request_count = self.proxy,self.request_count\n",
    "               \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "            # 爬取一篇\n",
    "            articleid,content,citation = self.crawl_onearticle_content(article_url,errorurl_path,cookie,proxy)\n",
    "            \n",
    "            # 检查是否是已经写入文献的errorurl.\n",
    "            if os.path.exists(errorurl_path):\n",
    "                 with open(errorurl_path,\"r\",encoding = \"utf-8\") as file:  \n",
    "                        errorurl = [url.strip() for url in file.readlines()]\n",
    "            else:\n",
    "                 with open(errorurl_path, 'w',encoding = \"utf-8\") as file:\n",
    "                        errorurl = []\n",
    "        \n",
    "            if pub_date and content:\n",
    "                # 检测是否爬取到的是付费墙内容\n",
    "                articleid,content,citation = self.check_content(article_url,errorurl_path,cookie,proxy,articleid,content,citation,verify_access_path)\n",
    "                if content:\n",
    "                    full_text = '\\n'.join([article_url,headline,abstract,author,pub_date,keywords,content,str(citation)])\n",
    "                    file_name = f\"{pub_date.split('T')[0] + '_' + str(articleid)}.txt\"\n",
    "                    file_path = os.path.join(directory_full, file_name)\n",
    "                    with open(file_path,\"w\",encoding = 'utf-8') as file:\n",
    "                        file.write(full_text) \n",
    "                else:\n",
    "                    pass\n",
    "                       \n",
    "            # 一部分链接可能不是www.nytimes开头的,将此部分写入no_pub_dateandcontent_path。\n",
    "            elif article_url not in errorurl:\n",
    "                with open(no_pub_dateandcontent_path,\"a\",encoding = \"utf-8\") as file:\n",
    "                    file.write(article_url + \"\\t\" + pub_date + \"\\t\" +  headline + \"\\n\")\n",
    "            else:\n",
    "                pass\n",
    "            request_count +=1\n",
    "            time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301c0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    nytimes_crawler_1983 = NYTimesCrawler(1983)\n",
    "    sort_data_1983  = nytimes_crawler_1983.open_metadatafile()[3041:][4082:][85:][14468:][5112:][10871:][28323:][8055:][14501:][16711:]\n",
    "    \n",
    "    nytimes_crawler_1984 = NYTimesCrawler(1984)\n",
    "    sort_data_1984  = nytimes_crawler_1984.open_metadatafile()[23152:][3631:][4187:][45:][14952:][5292:][11785:][30710:][7587:]\n",
    "    \n",
    "    nytimes_crawler_1985 = NYTimesCrawler(1985)\n",
    "    sort_data_1985  = nytimes_crawler_1985.open_metadatafile()[3768:][3993:][24954:][22391:][3660:][4224:][85:][13110:][5194:][10980:]\n",
    "    \n",
    "    nytimes_crawler_1986 = NYTimesCrawler(1986)\n",
    "    sort_data_1986  = nytimes_crawler_1986.open_metadatafile()[3752:][4896:][26763:][23542:][3663:][4521:][45:][14092:][5055:][10964:]\n",
    "    \n",
    "    nytimes_crawler_1987 = NYTimesCrawler(1987)\n",
    "    sort_data_1987  = nytimes_crawler_1987.open_metadatafile()[3739:][4614:][26817:][24363:][3640:][4212:][85:][13965:][5597:][11062:]\n",
    "    \n",
    "    nytimes_crawler_1988 = NYTimesCrawler(1988)\n",
    "    sort_data_1988  = nytimes_crawler_1988.open_metadatafile()[3416:][5215:][23695:][21727:][3235:][4536:][85:][12683:][5089:][11008:]\n",
    "    \n",
    "    nytimes_crawler_1989 = NYTimesCrawler(1989)\n",
    "    sort_data_1989  = nytimes_crawler_1989.open_metadatafile()[3289:][4088:][26325:][23089:][3201:][3787:][85:][14083:][4738:][10466:]\n",
    "    \n",
    "    nytimes_crawler_1990 = NYTimesCrawler(1990)\n",
    "    sort_data_1990  = nytimes_crawler_1990.open_metadatafile()[23905:][3750:][3577:][25929:][23133:][3636:][3628:][85:][11261:]\n",
    "    \n",
    "    nytimes_crawler_2009 = NYTimesCrawler(2009)\n",
    "    sort_data_2009  = nytimes_crawler_2009.open_metadatafile()[52551:][1925:][2981:][3733:][4250:][9464:][13665:][54112:]\n",
    "    \n",
    "    \n",
    "\n",
    "    thread1 = threading.Thread(target=nytimes_crawler_1983.crawl_nytime_main, args=(sort_data_1983,))\n",
    "    thread2 = threading.Thread(target=nytimes_crawler_1984.crawl_nytime_main, args=(sort_data_1984,))\n",
    "    thread3 = threading.Thread(target=nytimes_crawler_1985.crawl_nytime_main, args=(sort_data_1985,))\n",
    "    thread4 = threading.Thread(target=nytimes_crawler_1986.crawl_nytime_main, args=(sort_data_1986,))\n",
    "    thread5 = threading.Thread(target=nytimes_crawler_1987.crawl_nytime_main, args=(sort_data_1987,))\n",
    "    thread6 = threading.Thread(target=nytimes_crawler_1988.crawl_nytime_main, args=(sort_data_1988,))\n",
    "    thread7 = threading.Thread(target=nytimes_crawler_1989.crawl_nytime_main, args=(sort_data_1989,))\n",
    "    thread8 = threading.Thread(target=nytimes_crawler_1990.crawl_nytime_main, args=(sort_data_1990,))\n",
    "    thread9 = threading.Thread(target=nytimes_crawler_2009.crawl_nytime_main, args=(sort_data_2009,))\n",
    "    \n",
    "    \n",
    "    \n",
    "    启动线程\n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "    thread3.start()\n",
    "    thread4.start()\n",
    "    thread5.start()\n",
    "    thread6.start()\n",
    "    thread7.start()\n",
    "    thread8.start()\n",
    "    thread9.start()\n",
    "    thread10.start()\n",
    "\n",
    "    \n",
    "    等待线程完成\n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "    thread3.join()\n",
    "    thread4.join()\n",
    "    thread5.join()\n",
    "    thread6.join()\n",
    "    thread7.join()\n",
    "    thread8.join()\n",
    "    thread9.join()\n",
    "    thread10.join()\n",
    "    print(\"All threads have finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80281685",
   "metadata": {},
   "source": [
    "### 如果出现网络问题中断，查找中断元素索引，切片爬取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30897f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pub_date = \"2009-05-26\"\n",
    "nytimes_crawler_2009 = NYTimesCrawler(2009)\n",
    "sort_data_2009  = nytimes_crawler_2009.open_metadatafile() \n",
    "from datetime import datetime\n",
    "for index, item in enumerate(sort_data_2009):\n",
    "    if \"pub_date\" in item:\n",
    "        # 将pub_date字符串转换为datetime对象，以便比较\n",
    "        pub_date_datetime = datetime.strptime(item[\"pub_date\"], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        \n",
    "        # 检查是否匹配目标pub_date\n",
    "        if pub_date_datetime.strftime(\"%Y-%m-%d\") == target_pub_date:\n",
    "            # 找到匹配的元素和索引\n",
    "            print(\"找到匹配的元素：\", item)\n",
    "            print(\"元素索引：\", index)\n",
    "            break\n",
    "else:\n",
    "    # 没有找到匹配的元素\n",
    "    print(\"没有找到匹配的元素\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f533643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题\n",
    "#1.看看，cookies_pool是否有作用\n",
    "#2.有时间再探索一下，看看cookies对封锁是不是有帮助。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
